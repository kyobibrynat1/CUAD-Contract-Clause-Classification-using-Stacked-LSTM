{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad791394",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "- Install/import libraries\n",
    "- Set seeds\n",
    "- GPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33b06f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# If needed, install extras (uncomment)\n",
    "# %pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "# %pip install pandas numpy matplotlib scikit-learn tqdm\n",
    "\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, f1_score, precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "torch.backends.cudnn.benchmark = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4637e980",
   "metadata": {},
   "source": [
    "## 2. Load CUAD Dataset\n",
    "Assumes CUAD v1 JSON files are in `data/cuad/`.\n",
    "- `CUAD_v1.json` (contracts)\n",
    "- `CUAD_v1_questions.json` (41 clause types)\n",
    "- `CUAD_v1_annotations.json` (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c688f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('data/cuad')\n",
    "CONTRACTS_PATH = DATA_DIR / 'CUAD_v1.json'\n",
    "QUESTIONS_PATH = DATA_DIR / 'CUAD_v1_questions.json'\n",
    "ANNOTATIONS_PATH = DATA_DIR / 'CUAD_v1_annotations.json'\n",
    "\n",
    "def load_json(path: Path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "contracts = load_json(CONTRACTS_PATH)  # list of dicts with 'doc_id' and 'text'\n",
    "questions = load_json(QUESTIONS_PATH)  # list of clause definitions\n",
    "annotations = load_json(ANNOTATIONS_PATH)  # list of annotation objects\n",
    "\n",
    "print(f'Contracts: {len(contracts)} | Questions: {len(questions)} | Annotations: {len(annotations)}')\n",
    "\n",
    "clause_labels = [q['question_text'] for q in questions]\n",
    "label_to_idx = {label: i for i, label in enumerate(clause_labels)}\n",
    "num_labels = len(clause_labels)\n",
    "print('Num labels:', num_labels)\n",
    "\n",
    "# Build doc_id -> multi-hot labels\n",
    "doc_to_labels: Dict[str, List[int]] = defaultdict(lambda: [0] * num_labels)\n",
    "for ann in annotations:\n",
    "    doc_id = ann['doc_id']\n",
    "    q_text = ann['question_text']\n",
    "    if q_text not in label_to_idx:\n",
    "        continue\n",
    "    doc_to_labels[doc_id][label_to_idx[q_text]] = 1\n",
    "\n",
    "# Merge contracts with labels\n",
    "records = []\n",
    "for c in contracts:\n",
    "    doc_id = c['doc_id']\n",
    "    text = c.get('text', '')\n",
    "    labels = doc_to_labels[doc_id]\n",
    "    records.append({'doc_id': doc_id, 'text': text, 'labels': labels})\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(df.head())\n",
    "print('Label density (avg positives per doc):', np.mean([sum(l) for l in df.labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076d5520",
   "metadata": {},
   "source": [
    "## 3. Head–Tail Sampling Function\n",
    "Take first 1000 and last 1000 words, insert `[HT_SPLIT]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ff7a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "HT_SPLIT = '[HT_SPLIT]'\n",
    "HEAD_N = 1000\n",
    "TAIL_N = 1000\n",
    "MAX_SEQ_WORDS = HEAD_N + TAIL_N + 1  # include split token\n",
    "\n",
    "def simple_word_tokenize(text: str) -> List[str]:\n",
    "    return text.split()\n",
    "\n",
    "def head_tail_sample(text: str) -> List[str]:\n",
    "    tokens = simple_word_tokenize(text)\n",
    "    head = tokens[:HEAD_N]\n",
    "    tail = tokens[-TAIL_N:] if len(tokens) >= TAIL_N else tokens[-len(tokens):]\n",
    "    return head + [HT_SPLIT] + tail\n",
    "\n",
    "# Preview on first contract\n",
    "sample_tokens = head_tail_sample(df.iloc[0].text)\n",
    "print('Sample length:', len(sample_tokens))\n",
    "print('Snippet:', ' '.join(sample_tokens[:30]), '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb5a2f4",
   "metadata": {},
   "source": [
    "## 4. Tokenization + Numerical Encoding\n",
    "Build vocab from scratch and encode text; pad/truncate sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb695d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = '<pad>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "VOCAB_MIN_FREQ = 2\n",
    "MAX_SEQ_LEN = 2000 + 1  # should match MAX_SEQ_WORDS but kept explicit\n",
    "\n",
    "def build_vocab(tokenized_texts: List[List[str]], min_freq: int = VOCAB_MIN_FREQ) -> Dict[str, int]:\n",
    "    counter = Counter()\n",
    "    for toks in tokenized_texts:\n",
    "        counter.update(toks)\n",
    "    vocab = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "    for tok, freq in counter.items():\n",
    "        if freq >= min_freq and tok not in vocab:\n",
    "            vocab[tok] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def encode_tokens(tokens: List[str], vocab: Dict[str, int], max_len: int = MAX_SEQ_LEN) -> List[int]:\n",
    "    ids = [vocab.get(tok, vocab[UNK_TOKEN]) for tok in tokens[:max_len]]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [vocab[PAD_TOKEN]] * (max_len - len(ids))\n",
    "    return ids\n",
    "\n",
    "# Build tokenized corpus\n",
    "df['tokens'] = df.text.apply(head_tail_sample)\n",
    "vocab = build_vocab(df.tokens.tolist(), min_freq=VOCAB_MIN_FREQ)\n",
    "vocab_size = len(vocab)\n",
    "print('Vocab size:', vocab_size)\n",
    "\n",
    "df['input_ids'] = df.tokens.apply(lambda t: encode_tokens(t, vocab, MAX_SEQ_LEN))\n",
    "print('Encoded example length:', len(df.iloc[0].input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20274f51",
   "metadata": {},
   "source": [
    "## 5. Dataset + DataLoader\n",
    "Custom dataset, 70/15/15 split, DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e3baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CuadDataset(Dataset):\n",
    "    def __init__(self, df_slice: pd.DataFrame):\n",
    "        self.inputs = df_slice.input_ids.tolist()\n",
    "        self.labels = df_slice.labels.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.inputs[idx], dtype=torch.long)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return x, y\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.30, random_state=SEED, shuffle=True)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.50, random_state=SEED, shuffle=True)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_loader = DataLoader(CuadDataset(train_df), batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader = DataLoader(CuadDataset(val_df), batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "test_loader = DataLoader(CuadDataset(test_df), batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "len(train_loader), len(val_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a980a93",
   "metadata": {},
   "source": [
    "## 6. Model Architecture (Stacked LSTM)\n",
    "Embedding from scratch, 2–3 LSTM layers, dropout, sigmoid output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b214e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, num_layers: int, num_labels: int, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0.0, bidirectional=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        out, _ = self.lstm(emb)\n",
    "        pooled = out[:, -1, :]  # last timestep\n",
    "        logits = self.fc(self.dropout(pooled))\n",
    "        return self.activation(logits)\n",
    "\n",
    "model = StackedLSTMClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=200,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    num_labels=num_labels,\n",
    "    dropout=0.3,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bca92c",
   "metadata": {},
   "source": [
    "## 7. Training Loop\n",
    "Binary cross-entropy, optimizer choices (SGD/Adam/RMSprop), checkpoint best val F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06462024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_optimizer(name: str, params, lr: float):\n",
    "    name = name.lower()\n",
    "    if name == 'sgd':\n",
    "        return torch.optim.SGD(params, lr=lr, momentum=0.9)\n",
    "    if name == 'rmsprop':\n",
    "        return torch.optim.RMSprop(params, lr=lr)\n",
    "    return torch.optim.Adam(params, lr=lr)\n",
    "\n",
    "OPTIMIZER_NAME = 'adam'\n",
    "LR = 1e-3\n",
    "EPOCHS = 5\n",
    "\n",
    "optimizer = choose_optimizer(OPTIMIZER_NAME, model.parameters(), LR)\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in tqdm(loader, leave=False):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def eval_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(yb.cpu().numpy())\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    return total_loss / len(loader.dataset), all_preds, all_labels\n",
    "\n",
    "def threshold_outputs(preds, thresh=0.5):\n",
    "    return (preds >= thresh).astype(int)\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "history = {'train_loss': [], 'val_loss': [], 'val_f1': []}\n",
    "BEST_PATH = 'best_model.pt'\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_preds, val_labels = eval_epoch(model, val_loader, criterion, device)\n",
    "    val_bin = threshold_outputs(val_preds)\n",
    "    val_f1 = f1_score(val_labels, val_bin, average='micro', zero_division=0)\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_f1'].append(val_f1)\n",
    "\n",
    "    print(f'Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_f1={val_f1:.4f}')\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), BEST_PATH)\n",
    "        print('Saved new best model')\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(history['train_loss'], label='train_loss')\n",
    "plt.plot(history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(history['val_f1'], label='val_f1')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 (micro)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87ee78b",
   "metadata": {},
   "source": [
    "## 8. Evaluation\n",
    "Per-class metrics and overall scores; optional confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d436b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Load best model if checkpoint exists (otherwise use current weights)\n",
    "if Path(BEST_PATH).exists():\n",
    "    model.load_state_dict(torch.load(BEST_PATH, map_location=device))\n",
    "    print(f\"Loaded checkpoint: {BEST_PATH}\")\n",
    "else:\n",
    "    print(f\"Checkpoint {BEST_PATH} not found. Using current model weights.\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "test_loss, test_preds, test_labels = eval_epoch(model, test_loader, criterion, device)\n",
    "test_bin = threshold_outputs(test_preds)\n",
    "\n",
    "overall_precision, overall_recall, overall_f1, _ = precision_recall_fscore_support(\n",
    "    test_labels, test_bin, average='micro', zero_division=0\n",
    ")\n",
    "overall_acc = accuracy_score(test_labels.flatten(), test_bin.flatten())\n",
    "\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "print(\n",
    "    f\"Accuracy: {overall_acc:.4f}  Precision: {overall_precision:.4f}  \"\n",
    "    f\"Recall: {overall_recall:.4f}  F1: {overall_f1:.4f}\"\n",
    ")\n",
    "\n",
    "report = classification_report(test_labels, test_bin, target_names=clause_labels, zero_division=0)\n",
    "print(report)\n",
    "\n",
    "# Optional: multilabel confusion matrix (sklearn supports per-class)\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "cm = multilabel_confusion_matrix(test_labels, test_bin)\n",
    "print('Confusion matrix shape:', cm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0546c54e",
   "metadata": {},
   "source": [
    "## 9. Ablation Setup (Optional)\n",
    "Compare head-only, tail-only, head–tail, simple truncation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390172f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_variant_tokens(text: str, variant: str) -> List[str]:\n",
    "    toks = simple_word_tokenize(text)\n",
    "    if variant == 'head':\n",
    "        return toks[:HEAD_N]\n",
    "    if variant == 'tail':\n",
    "        return toks[-TAIL_N:]\n",
    "    if variant == 'truncate':\n",
    "        return toks[:MAX_SEQ_WORDS]\n",
    "    return head_tail_sample(text)\n",
    "\n",
    "def prepare_variant_df(df_in: pd.DataFrame, variant: str) -> Tuple[pd.DataFrame, Dict[str, int]]:\n",
    "    df_copy = df_in.copy()\n",
    "    df_copy['tokens'] = df_copy.text.apply(lambda t: make_variant_tokens(t, variant))\n",
    "    vocab_var = build_vocab(df_copy.tokens.tolist(), min_freq=VOCAB_MIN_FREQ)\n",
    "    df_copy['input_ids'] = df_copy.tokens.apply(lambda t: encode_tokens(t, vocab_var, MAX_SEQ_LEN))\n",
    "    return df_copy, vocab_var\n",
    "\n",
    "# Example: head-only dataset (demo; re-train with same pipeline for proper comparison)\n",
    "head_df, head_vocab = prepare_variant_df(df, 'head')\n",
    "print('Head-only vocab size:', len(head_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049f4ca9",
   "metadata": {},
   "source": [
    "## 10. Inference on a Sample Contract\n",
    "Load best checkpoint, run prediction for a single contract string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fb1dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text: str, model: nn.Module, vocab: Dict[str, int], threshold: float = 0.5):\n",
    "    tokens = head_tail_sample(text)\n",
    "    ids = torch.tensor([encode_tokens(tokens, vocab, MAX_SEQ_LEN)], dtype=torch.long).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = model(ids).cpu().numpy()[0]\n",
    "    pred_idxs = [i for i, p in enumerate(probs) if p >= threshold]\n",
    "    return [(clause_labels[i], probs[i]) for i in pred_idxs]\n",
    "\n",
    "sample_text = df.iloc[0].text\n",
    "preds = predict_text(sample_text, model, vocab, threshold=0.5)\n",
    "print('Predicted clauses:')\n",
    "for label, prob in preds:\n",
    "    print(f'{label}: {prob:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
