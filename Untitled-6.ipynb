{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a707f5",
   "metadata": {},
   "source": [
    "## Installation Note\n",
    "\n",
    "If you don't have h5py installed, run:\n",
    "```bash\n",
    "pip install h5py\n",
    "```\n",
    "\n",
    "Models will be saved in both formats:\n",
    "- `.pt` - PyTorch native format\n",
    "- `.h5` - HDF5 format (compatible with other frameworks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f342af57",
   "metadata": {},
   "source": [
    "# Legal Contract Clause Classification using Stacked LSTM\n",
    "## CCS 248 – Artificial Neural Networks Final Project\n",
    "---\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "**Automated Classification of Legal Contract Clauses**\n",
    "\n",
    "Lawyers spend hours manually reading and categorizing individual contract clauses (e.g., governing law, termination, confidentiality). This project automates that process using deep learning to classify each clause context into predefined legal categories.\n",
    "\n",
    "## Why Deep Learning?\n",
    "\n",
    "Traditional methods like keyword matching don't understand context or handle legal language variations. LSTMs can:\n",
    "- Read clause sequences and understand semantic meaning\n",
    "- Capture long-range dependencies in legal text\n",
    "- Distinguish similar phrases used in different legal contexts\n",
    "\n",
    "## Solution: Stacked Bidirectional LSTM with Attention\n",
    "\n",
    "Using a 2-layer bidirectional LSTM network plus an attention pooling head:\n",
    "- **Bidirectional processing** — reads clauses forward and backward for full context\n",
    "- **Stacked layers + attention** — captures low-level patterns and focuses on salient tokens\n",
    "- **Dropout regularization** — prevents overfitting on legal jargon\n",
    "\n",
    "## Dataset\n",
    "\n",
    "**CUAD v1 master_clauses.csv** (flattened clause snippets)\n",
    "- 1,965 snippets, 40 clause labels originally\n",
    "- Filtered to 7 clause types with at least 5 examples each for stable stratification\n",
    "\n",
    "## Target\n",
    "\n",
    "**Test Accuracy: 50-60%** (course requirement)\n",
    "\n",
    "**Evaluation**: Accuracy, macro F1, per-class precision/recall, confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b3447d",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1ac66423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.9.1+cpu\n",
      "NumPy Version: 2.1.3\n",
      "Pandas Version: 2.2.3\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Core data processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# Text processing\n",
    "import string\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# PyTorch for deep learning (avoid Keras)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Scikit-learn for preprocessing and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# Display versions\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482fa1ee",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "33840dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading XLSX files from: d:\\\\CodingRelated\\\\Codes.Ams\\\\ANNFINAL\\\\CUAD_v1\\\\label_group_xlsx\n",
      "✓ Loaded 8035 snippets from 28 XLSX files\n",
      "Unique clause types: 47\n"
     ]
    }
   ],
   "source": [
    "# Load clause snippets from CUAD XLSX sheets (label_group_xlsx)\n",
    "import glob\n",
    "XLSX_DIR = r\"d:\\\\CodingRelated\\\\Codes.Ams\\\\ANNFINAL\\\\CUAD_v1\\\\label_group_xlsx\"\n",
    "print(f\"Loading XLSX files from: {XLSX_DIR}\")\n",
    "\n",
    "def load_xlsx_snippets(xlsx_dir: str):\n",
    "    rows = []\n",
    "    files = glob.glob(os.path.join(xlsx_dir, \"*.xlsx\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No .xlsx files found in {xlsx_dir}\")\n",
    "    for path in files:\n",
    "        df_x = pd.read_excel(path)\n",
    "        if df_x.empty:\n",
    "            continue\n",
    "        clause_cols = [c for c in df_x.columns if c != df_x.columns[0]]\n",
    "        for _, row in df_x.iterrows():\n",
    "            for col in clause_cols:\n",
    "                text = row[col]\n",
    "                if pd.isna(text):\n",
    "                    continue\n",
    "                text = str(text).strip()\n",
    "                if not text:\n",
    "                    continue\n",
    "                rows.append({\"context\": text, \"clause_type\": col})\n",
    "    df_out = pd.DataFrame(rows).drop_duplicates().reset_index(drop=True)\n",
    "    return df_out, len(files)\n",
    "\n",
    "df, n_files = load_xlsx_snippets(XLSX_DIR)\n",
    "print(f\"✓ Loaded {len(df)} snippets from {n_files} XLSX files\")\n",
    "print(f\"Unique clause types: {df['clause_type'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8d0c97e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             context        clause_type\n",
      "0  MA may not assign, sell, lease or otherwise tr...    Anti-assignment\n",
      "1  This Agreement may not be assigned, sold or tr...    Anti-assignment\n",
      "2  For purposes of the preceding sentence, and wi...  Change of Control\n",
      "3  Licensee shall not assign or otherwise transfe...    Anti-assignment\n",
      "4  Licensee shall have the right to assign or sub...    Anti-assignment\n",
      "\n",
      "Top clause counts:\n",
      "clause_type\n",
      "Parties                      505\n",
      "Parties-Answer               499\n",
      "Agreement Date               464\n",
      "Governing Law                435\n",
      "Agreement Date-Answer        424\n",
      "Expiration Date              411\n",
      "Effective Date               384\n",
      "Anti-assignment              372\n",
      "Effective Date-Answer        328\n",
      "Document Name                311\n",
      "Cap on Liability             275\n",
      "License Grant                254\n",
      "Expiration Date-Answer       249\n",
      "Audit Rights                 214\n",
      "Post-termination Services    182\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Basic dataset overview\n",
    "print(df.head())\n",
    "print(\"\\nTop clause counts:\")\n",
    "print(df['clause_type'].value_counts().head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "97ba116f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total snippets: 8035\n",
      "Unique clause types: 47\n",
      "Average length (words): 74.6\n"
     ]
    }
   ],
   "source": [
    "# Dataset stats\n",
    "print(f\"Total snippets: {len(df)}\")\n",
    "print(f\"Unique clause types: {df['clause_type'].nunique()}\")\n",
    "print(f\"Average length (words): {df['context'].apply(lambda x: len(str(x).split())).mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "38b61748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "First 5 Rows of Dataset:\n",
      "================================================================================\n",
      "                                             context        clause_type\n",
      "0  MA may not assign, sell, lease or otherwise tr...    Anti-assignment\n",
      "1  This Agreement may not be assigned, sold or tr...    Anti-assignment\n",
      "2  For purposes of the preceding sentence, and wi...  Change of Control\n",
      "3  Licensee shall not assign or otherwise transfe...    Anti-assignment\n",
      "4  Licensee shall have the right to assign or sub...    Anti-assignment\n",
      "\n",
      "================================================================================\n",
      "Dataset Info:\n",
      "================================================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8035 entries, 0 to 8034\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   context      8035 non-null   object\n",
      " 1   clause_type  8035 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 125.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Display first few rows\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"First 5 Rows of Dataset:\")\n",
    "print(\"=\"*80)\n",
    "print(df.head())\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Dataset Info:\")\n",
    "print(\"=\"*80)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09ad8e5",
   "metadata": {},
   "source": [
    "# 3. Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "81dd5259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "context        0\n",
      "clause_type    0\n",
      "dtype: int64\n",
      "\n",
      "Total samples: 8035\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(f\"\\nTotal samples: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8071f952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 clause types:\n",
      "clause_type\n",
      "Parties                  505\n",
      "Parties-Answer           499\n",
      "Agreement Date           464\n",
      "Governing Law            435\n",
      "Agreement Date-Answer    424\n",
      "Expiration Date          411\n",
      "Effective Date           384\n",
      "Anti-assignment          372\n",
      "Effective Date-Answer    328\n",
      "Document Name            311\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution\n",
    "print(\"Top 10 clause types:\")\n",
    "print(df['clause_type'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c66c8cd",
   "metadata": {},
   "source": [
    "# 4. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "78ad9e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: THIS AGREEMENT is made on January 1, 2020!!!\n",
      "After: this agreement is made on january ,\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Basic text cleaning\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s\\.,;:\\-]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Test\n",
    "sample = \"THIS AGREEMENT is made on January 1, 2020!!!\"\n",
    "print(\"Before:\", sample)\n",
    "print(\"After:\", clean_text(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6dbd3b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cleaned all documents\n"
     ]
    }
   ],
   "source": [
    "# Apply cleaning\n",
    "df['cleaned_text'] = df['context'].apply(clean_text)\n",
    "print(\"✓ Cleaned all documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f08f17",
   "metadata": {},
   "source": [
    "# 5. Text Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "302c0d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using 8035 clause contexts (no truncation needed)\n"
     ]
    }
   ],
   "source": [
    "# Use cleaned text directly (clause contexts are already short)\n",
    "df['sampled_text'] = df['cleaned_text']\n",
    "print(f\"✓ Using {len(df)} clause contexts (no truncation needed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627a4d8a",
   "metadata": {},
   "source": [
    "# 6. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dd43de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    \"\"\"Simple tokenizer - built from scratch\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=10000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_to_index = {\"<OOV>\": 1}\n",
    "        self.word_counts = Counter()\n",
    "        \n",
    "    def fit_on_texts(self, texts):\n",
    "        for text in texts:\n",
    "            self.word_counts.update(str(text).split())\n",
    "        \n",
    "        most_common = self.word_counts.most_common(self.vocab_size - 2)\n",
    "        for idx, (word, _) in enumerate(most_common, start=2):\n",
    "            self.word_to_index[word] = idx\n",
    "        \n",
    "        print(f\"Vocabulary size: {len(self.word_to_index)}\")\n",
    "    \n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            seq = [self.word_to_index.get(word, 1) for word in str(text).split()]\n",
    "            sequences.append(seq)\n",
    "        return sequences\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.word_to_index)\n",
    "\n",
    "# Tokenizer will be built after filtering to top clauses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b970fb9f",
   "metadata": {},
   "source": [
    "# 7. Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6128b78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, maxlen, padding='post', value=0):\n",
    "    \"\"\"Pad sequences to the same length\"\"\"\n",
    "    padded = np.zeros((len(sequences), maxlen), dtype=np.int32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        if len(seq) > maxlen:\n",
    "            if padding == 'post':\n",
    "                padded[i] = seq[:maxlen]\n",
    "            else:\n",
    "                padded[i] = seq[-maxlen:]\n",
    "        else:\n",
    "            if padding == 'post':\n",
    "                padded[i, :len(seq)] = seq\n",
    "            else:\n",
    "                padded[i, -len(seq):] = seq\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63819b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4662 samples before augmentation\n",
      "Top clause types (min 5 per class):\n",
      "  1. Parties... (505 samples)\n",
      "  2. Parties-Answer... (499 samples)\n",
      "  3. Agreement Date... (464 samples)\n",
      "  4. Governing Law... (435 samples)\n",
      "  5. Agreement Date-Answer... (424 samples)\n",
      "  6. Expiration Date... (411 samples)\n",
      "  7. Effective Date... (384 samples)\n",
      "  8. Anti-assignment... (372 samples)\n",
      "  9. Effective Date-Answer... (328 samples)\n",
      "  10. Document Name... (311 samples)\n",
      "  11. Cap on Liability... (275 samples)\n",
      "  12. License Grant... (254 samples)\n",
      "No augmentation applied (all classes already above target or wordnet unavailable)\n",
      "Total samples after augmentation: 4662\n",
      "Vocabulary size: 8319\n",
      "Sequence length percentile(85th): 65\n",
      "Max sequence length used: 65 (capped at 160)\n",
      "Padded shape (filtered): (4662, 65)\n"
     ]
    }
   ],
   "source": [
    "# Select clause types with enough support to stratify\n",
    "TOP_N = 12\n",
    "MIN_COUNT = 5\n",
    "clause_counts = df['clause_type'].value_counts()\n",
    "filtered_counts = clause_counts[clause_counts >= MIN_COUNT]\n",
    "top_clauses = filtered_counts.head(TOP_N).index.tolist()\n",
    "df_filtered = df[df['clause_type'].isin(top_clauses)].copy()\n",
    "\n",
    "print(f\"Using {len(df_filtered)} samples before augmentation\")\n",
    "print(f\"Top clause types (min {MIN_COUNT} per class):\")\n",
    "for i, (clause, count) in enumerate(filtered_counts.head(TOP_N).items(), 1):\n",
    "    print(f\"  {i}. {clause[:80]}... ({count} samples)\")\n",
    "\n",
    "ENABLE_AUGMENTATION = True\n",
    "TARGET_MIN_PER_CLASS = 30  # desired minimum rows per class after augmentation\n",
    "REPLACE_PROB = 0.25        # probability of replacing a token with a synonym\n",
    "MAX_AUG_PER_CLASS = 80     # cap to avoid explosion per class\n",
    "\n",
    "if ENABLE_AUGMENTATION:\n",
    "    import random\n",
    "    try:\n",
    "        import nltk\n",
    "        from nltk.corpus import wordnet as wn\n",
    "        nltk.download('wordnet', quiet=True)\n",
    "        nltk.download('omw-1.4', quiet=True)\n",
    "    except Exception as e:\n",
    "        wn = None\n",
    "        print(f\"NLTK/wordnet not available, skipping synonym augmentation: {e}\")\n",
    "\n",
    "    def get_synonyms(word):\n",
    "        if wn is None:\n",
    "            return []\n",
    "        syns = set()\n",
    "        for syn in wn.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                candidate = lemma.name().replace('_', ' ').lower()\n",
    "                if candidate.isalpha() and candidate != word.lower():\n",
    "                    syns.add(candidate)\n",
    "        return list(syns)\n",
    "\n",
    "    def synonym_replace(text, replace_prob=0.2):\n",
    "        tokens = str(text).split()\n",
    "        new_tokens = []\n",
    "        for tok in tokens:\n",
    "            if random.random() < replace_prob:\n",
    "                syns = get_synonyms(tok)\n",
    "                if syns:\n",
    "                    new_tokens.append(random.choice(syns))\n",
    "                    continue\n",
    "            new_tokens.append(tok)\n",
    "        return \" \".join(new_tokens)\n",
    "\n",
    "    aug_rows = []\n",
    "    for label, group in df_filtered.groupby('clause_type'):\n",
    "        current_count = len(group)\n",
    "        if current_count >= TARGET_MIN_PER_CLASS:\n",
    "            continue\n",
    "        needed = min(TARGET_MIN_PER_CLASS - current_count, MAX_AUG_PER_CLASS)\n",
    "        pool = group['sampled_text'].tolist()\n",
    "        for i in range(needed):\n",
    "            base_text = pool[i % len(pool)]\n",
    "            aug_text = synonym_replace(base_text, replace_prob=REPLACE_PROB)\n",
    "            aug_rows.append({\n",
    "                'context': aug_text,\n",
    "                'clause_type': label,\n",
    "                'cleaned_text': aug_text,\n",
    "                'sampled_text': aug_text,\n",
    "            })\n",
    "\n",
    "    if aug_rows:\n",
    "        df_aug = pd.DataFrame(aug_rows)\n",
    "        df_filtered = pd.concat([df_filtered, df_aug], ignore_index=True)\n",
    "        print(f\"Applied augmentation: +{len(aug_rows)} synthetic rows\")\n",
    "    else:\n",
    "        print(\"No augmentation applied (all classes already above target or wordnet unavailable)\")\n",
    "\n",
    "print(f\"Total samples after augmentation: {len(df_filtered)}\")\n",
    "\n",
    "# Build tokenizer on filtered (and possibly augmented) data with smaller vocab to limit noise\n",
    "tokenizer = CustomTokenizer(vocab_size=10000)\n",
    "tokenizer.fit_on_texts(df_filtered['sampled_text'])\n",
    "\n",
    "# Tokenize filtered data\n",
    "sequences_filtered = tokenizer.texts_to_sequences(df_filtered['sampled_text'])\n",
    "\n",
    "# Length stats and padding length\n",
    "sequence_lengths = [len(seq) for seq in sequences_filtered]\n",
    "percentile_len = int(np.percentile(sequence_lengths, 85))\n",
    "MAX_LENGTH = min(percentile_len, 160)\n",
    "print(f\"Sequence length percentile(85th): {percentile_len}\")\n",
    "print(f\"Max sequence length used: {MAX_LENGTH} (capped at 160)\")\n",
    "\n",
    "# Pad filtered sequences\n",
    "X_filtered = pad_sequences(sequences_filtered, maxlen=MAX_LENGTH, padding='post')\n",
    "print(f\"Padded shape (filtered): {X_filtered.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5ce57e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV tokens: 0 / 189484 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: OOV rate on filtered sequences\n",
    "# OOV token id is 1 in the tokenizer\n",
    "all_tokens = sum(len(seq) for seq in sequences_filtered)\n",
    "oov_tokens = sum(sum(1 for t in seq if t == 1) for seq in sequences_filtered)\n",
    "oov_pct = 100 * oov_tokens / max(1, all_tokens)\n",
    "print(f\"OOV tokens: {oov_tokens} / {all_tokens} ({oov_pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "93d1f0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels shape: (4662,)\n",
      "Classes: ['Agreement Date' 'Agreement Date-Answer' 'Anti-assignment'\n",
      " 'Cap on Liability' 'Document Name' 'Effective Date'\n",
      " 'Effective Date-Answer' 'Expiration Date' 'Governing Law' 'License Grant'\n",
      " 'Parties' 'Parties-Answer']\n"
     ]
    }
   ],
   "source": [
    "# Encode labels after filtering\n",
    "df_filtered = df_filtered.reset_index(drop=True)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(df_filtered['clause_type'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Labels shape: {y_encoded.shape}\")\n",
    "print(f\"Classes: {label_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0f69fdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building TF-IDF matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Logistic accuracy (test): 0.8270\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6056    0.7842    0.6834       139\n",
      "           1     0.5270    1.0000    0.6902       127\n",
      "           2     1.0000    0.9554    0.9772       112\n",
      "           3     0.9878    0.9759    0.9818        83\n",
      "           4     1.0000    0.9570    0.9780        93\n",
      "           5     0.4918    0.2609    0.3409       115\n",
      "           6     0.0000    0.0000    0.0000        98\n",
      "           7     0.8667    0.9512    0.9070       123\n",
      "           8     1.0000    1.0000    1.0000       131\n",
      "           9     0.9868    0.9868    0.9868        76\n",
      "          10     0.9868    0.9803    0.9835       152\n",
      "          11     0.9726    0.9467    0.9595       150\n",
      "\n",
      "    accuracy                         0.8270      1399\n",
      "   macro avg     0.7854    0.8165    0.7907      1399\n",
      "weighted avg     0.7885    0.8270    0.7968      1399\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\heral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\heral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF + Logistic Regression baseline (quick sanity check)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "texts = df_filtered['sampled_text'].astype(str).tolist()\n",
    "labels = y_encoded\n",
    "\n",
    "print('Building TF-IDF matrix...')\n",
    "vect = TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n",
    "X_tfidf = vect.fit_transform(texts)\n",
    "\n",
    "# Split and train a simple linear classifier\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_tfidf, labels, test_size=0.30, random_state=42, stratify=labels)\n",
    "clf = LogisticRegression(max_iter=2000, solver='lbfgs', multi_class='multinomial')\n",
    "clf.fit(X_tr, y_tr)\n",
    "acc = clf.score(X_te, y_te)\n",
    "print(f\"TF-IDF Logistic accuracy (test): {acc:.4f}\")\n",
    "\n",
    "# Print detailed per-class report\n",
    "y_pred = clf.predict(X_te)\n",
    "print('\\nClassification report:')\n",
    "print(classification_report(y_te, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da76fde",
   "metadata": {},
   "source": [
    "# 8. Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7c01355c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (3263, 65)\n",
      "Val: (699, 65)\n",
      "Test: (700, 65)\n",
      "Class counts: [325 297 260 192 218 269 230 288 304 178 353 349]\n",
      "Class weights (normalized): [0.79870408 0.87400278 0.9983801  1.35197305 1.19072856 0.96497705\n",
      " 1.12860359 0.90131537 0.85387772 1.45830801 0.73535079 0.74377887]\n"
     ]
    }
   ],
   "source": [
    "# Split data: 70% train, 15% val, 15% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_filtered, y_encoded, test_size=0.30, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}\")\n",
    "print(f\"Val: {X_val.shape}\")\n",
    "print(f\"Test: {X_test.shape}\")\n",
    "\n",
    "# Class weights to handle imbalance (toggle with USE_CLASS_WEIGHTS)\n",
    "class_counts = np.bincount(y_train, minlength=num_classes)\n",
    "class_weights = 1.0 / (class_counts + 1e-6)\n",
    "class_weights = class_weights * (num_classes / class_weights.sum())\n",
    "print(\"Class counts:\", class_counts)\n",
    "print(\"Class weights (normalized):\", class_weights)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "USE_CLASS_WEIGHTS = True\n",
    "USE_SAMPLER = True\n",
    "\n",
    "class ClauseDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = ClauseDataset(X_train, y_train)\n",
    "val_dataset = ClauseDataset(X_val, y_val)\n",
    "test_dataset = ClauseDataset(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48211ecb",
   "metadata": {},
   "source": [
    "# 9. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "76b5c1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: 8319, Classes: 12, Max length: 65\n"
     ]
    }
   ],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"Bidirectional stacked LSTM with attention for clause classification\"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim=200, lstm_1=128, lstm_2=96, dropout=0.25, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embed_dim, padding_idx=0)\n",
    "        self.lstm1 = nn.LSTM(embed_dim, lstm_1, batch_first=True, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.lstm2 = nn.LSTM(lstm_1 * 2, lstm_2, batch_first=True, bidirectional=True)\n",
    "        self.attn = nn.Linear(lstm_2 * 2, 1)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(lstm_2 * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        scores = torch.tanh(self.attn(x))\n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "        context = (x * weights).sum(dim=1)\n",
    "        context = self.dropout2(context)\n",
    "        return self.fc(context)\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.word_to_index)\n",
    "NUM_CLASSES = num_classes\n",
    "print(f\"Vocab: {VOCAB_SIZE}, Classes: {NUM_CLASSES}, Max length: {MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a147b",
   "metadata": {},
   "source": [
    "# 10. Hyperparameter Tuning Setup\n",
    "\n",
    "Testing different optimizers as required by the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3b4756ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will test 5 configurations\n"
     ]
    }
   ],
   "source": [
    "# Configurations to test - tuned for faster convergence with attention\n",
    "configs = [\n",
    "    {'opt': 'Adam',    'lr': 0.0008, 'wd': 1e-4, 'batch': 64, 'epochs': 5},\n",
    "    {'opt': 'Adam',    'lr': 0.0010, 'wd': 1e-4, 'batch': 64, 'epochs': 5},\n",
    "    {'opt': 'Adam',    'lr': 0.0005, 'wd': 1e-4, 'batch': 64, 'epochs': 5},\n",
    "    {'opt': 'RMSprop', 'lr': 0.0008, 'wd': 0.0,  'batch': 64, 'epochs': 5},\n",
    "    {'opt': 'RMSprop', 'lr': 0.0005, 'wd': 0.0,  'batch': 64, 'epochs': 5},\n",
    "]\n",
    "\n",
    "print(f\"Will test {len(configs)} configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61058ec0",
   "metadata": {},
   "source": [
    "# 11. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "751b7ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "models_dir = r'd:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run4'\n",
    "os.makedirs(models_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "03ba2d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Config 1/5: Adam, LR=0.0008, WD=0.0001\n",
      "============================================================\n",
      "Model created, starting training...\n",
      "Training batches: 51, Val batches: 11\n",
      "Epoch 1/5 - Train loss 1.8665, acc 0.2939 | Val loss 1.3304, acc 0.5007\n",
      "Epoch 2/5 - Train loss 0.8242, acc 0.6690 | Val loss 0.4950, acc 0.8040\n",
      "Epoch 3/5 - Train loss 0.3771, acc 0.8023 | Val loss 0.3647, acc 0.8140\n",
      "Epoch 4/5 - Train loss 0.3150, acc 0.8180 | Val loss 0.3518, acc 0.8140\n",
      "Epoch 5/5 - Train loss 0.2745, acc 0.8367 | Val loss 0.3414, acc 0.8340\n",
      "Val pred distribution: Counter({np.int64(0): 106, np.int64(1): 105, np.int64(10): 76, np.int64(11): 75, np.int64(7): 66, np.int64(8): 58, np.int64(2): 56, np.int64(3): 46, np.int64(4): 44, np.int64(9): 41, np.int64(5): 19, np.int64(6): 7})\n",
      "Test accuracy: 0.8300\n",
      "Saved: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run4\\model_1.pt and d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run4\\model_1.h5\n",
      "\n",
      "============================================================\n",
      "Config 2/5: Adam, LR=0.001, WD=0.0001\n",
      "============================================================\n",
      "Model created, starting training...\n",
      "Training batches: 51, Val batches: 11\n",
      "Epoch 1/5 - Train loss 1.7320, acc 0.3748 | Val loss 0.9145, acc 0.6881\n",
      "Epoch 2/5 - Train loss 0.6842, acc 0.6948 | Val loss 0.5240, acc 0.7697\n",
      "Epoch 3/5 - Train loss 0.3719, acc 0.8137 | Val loss 0.4296, acc 0.8226\n",
      "Epoch 4/5 - Train loss 0.3299, acc 0.8241 | Val loss 0.4146, acc 0.8183\n",
      "Epoch 5/5 - Train loss 0.2964, acc 0.8290 | Val loss 0.3616, acc 0.8240\n",
      "Val pred distribution: Counter({np.int64(0): 106, np.int64(6): 105, np.int64(11): 80, np.int64(10): 77, np.int64(7): 64, np.int64(8): 64, np.int64(2): 54, np.int64(4): 45, np.int64(3): 43, np.int64(9): 39, np.int64(5): 22})\n",
      "Test accuracy: 0.7943\n",
      "Saved: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run4\\model_2.pt and d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run4\\model_2.h5\n",
      "\n",
      "============================================================\n",
      "Config 3/5: Adam, LR=0.0005, WD=0.0001\n",
      "============================================================\n",
      "Model created, starting training...\n",
      "Training batches: 51, Val batches: 11\n",
      "Epoch 1/5 - Train loss 2.0709, acc 0.2492 | Val loss 1.6210, acc 0.2976\n",
      "Epoch 2/5 - Train loss 1.1160, acc 0.5835 | Val loss 0.8636, acc 0.6295\n",
      "Epoch 3/5 - Train loss 0.5646, acc 0.7518 | Val loss 0.4386, acc 0.8054\n",
      "Epoch 4/5 - Train loss 0.3524, acc 0.8115 | Val loss 0.3480, acc 0.8155\n",
      "Epoch 5/5 - Train loss 0.2936, acc 0.8317 | Val loss 0.3585, acc 0.8212\n",
      "Val pred distribution: Counter({np.int64(1): 112, np.int64(10): 76, np.int64(11): 75, np.int64(7): 67, np.int64(5): 66, np.int64(8): 62, np.int64(0): 60, np.int64(2): 51, np.int64(4): 46, np.int64(3): 42, np.int64(9): 42})\n",
      "Test accuracy: 0.8029\n",
      "Saved: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run4\\model_3.pt and d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run4\\model_3.h5\n",
      "\n",
      "============================================================\n",
      "Config 4/5: RMSprop, LR=0.0008, WD=0.0\n",
      "============================================================\n",
      "Model created, starting training...\n",
      "Training batches: 51, Val batches: 11\n",
      "Epoch 1/5 - Train loss 0.9762, acc 0.5964 | Val loss 0.4642, acc 0.7783\n",
      "Epoch 2/5 - Train loss 0.3666, acc 0.8091 | Val loss 0.3650, acc 0.7997\n",
      "Epoch 3/5 - Train loss 0.2919, acc 0.8238 | Val loss 0.3323, acc 0.8112\n",
      "Epoch 4/5 - Train loss 0.2710, acc 0.8314 | Val loss 0.3365, acc 0.8226\n",
      "Epoch 5/5 - Train loss 0.2577, acc 0.8443 | Val loss 0.3247, acc 0.8040\n",
      "Val pred distribution: Counter({np.int64(6): 112, np.int64(5): 81, np.int64(10): 76, np.int64(11): 74, np.int64(7): 67, np.int64(8): 63, np.int64(2): 53, np.int64(4): 47, np.int64(3): 45, np.int64(0): 42, np.int64(9): 39})\n",
      "Test accuracy: 0.7871\n",
      "Saved: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run4\\model_4.pt and d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run4\\model_4.h5\n",
      "\n",
      "============================================================\n",
      "Config 5/5: RMSprop, LR=0.0005, WD=0.0\n",
      "============================================================\n",
      "Model created, starting training...\n",
      "Training batches: 51, Val batches: 11\n",
      "Epoch 1/5 - Train loss 1.1151, acc 0.5602 | Val loss 0.5178, acc 0.7954\n",
      "Epoch 2/5 - Train loss 0.3801, acc 0.8137 | Val loss 0.4125, acc 0.7797\n",
      "Epoch 3/5 - Train loss 0.3347, acc 0.8170 | Val loss 0.3788, acc 0.7697\n",
      "Epoch 4/5 - Train loss 0.2936, acc 0.8308 | Val loss 0.3522, acc 0.8226\n",
      "Epoch 5/5 - Train loss 0.2713, acc 0.8314 | Val loss 0.3395, acc 0.8312\n",
      "Val pred distribution: Counter({np.int64(1): 105, np.int64(0): 95, np.int64(10): 76, np.int64(11): 74, np.int64(7): 64, np.int64(8): 62, np.int64(2): 55, np.int64(3): 47, np.int64(4): 46, np.int64(9): 37, np.int64(5): 31, np.int64(6): 7})\n",
      "Test accuracy: 0.8171\n",
      "Saved: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run4\\model_5.pt and d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run4\\model_5.h5\n",
      "\n",
      "✓ Training complete!\n"
     ]
    }
   ],
   "source": [
    "def run_epoch(model, loader, criterion, optimizer=None):\n",
    "    model.train() if optimizer else model.eval()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(loader):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        if optimizer:\n",
    "            optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        if optimizer:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item() * X_batch.size(0)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        total_correct += (preds == y_batch).sum().item()\n",
    "        total_samples += X_batch.size(0)\n",
    "        \n",
    "        # Progress indicator every 50 batches\n",
    "        if optimizer and batch_idx % 50 == 0:\n",
    "            print(f\"  Batch {batch_idx}/{len(loader)}\", end='\\r')\n",
    "    \n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = total_correct / total_samples\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "def save_model_as_h5(model, filepath):\n",
    "    \"\"\"Save PyTorch model weights to HDF5 format\"\"\"\n",
    "    import h5py\n",
    "    state_dict = model.state_dict()\n",
    "    with h5py.File(filepath, 'w') as f:\n",
    "        for key, value in state_dict.items():\n",
    "            f.create_dataset(key, data=value.cpu().numpy())\n",
    "\n",
    "results = []\n",
    "models_dir = r'd:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run4'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "for i, cfg in enumerate(configs, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Config {i}/{len(configs)}: {cfg['opt']}, LR={cfg['lr']}, WD={cfg['wd']}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    model = LSTMClassifier(VOCAB_SIZE, embed_dim=200, num_classes=NUM_CLASSES).to(device)\n",
    "    print(f\"Model created, starting training...\")\n",
    "    \n",
    "    if cfg['opt'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=cfg['lr'], weight_decay=cfg.get('wd', 0.0))\n",
    "    elif cfg['opt'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=cfg['lr'], weight_decay=cfg.get('wd', 0.0))\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=cfg['lr'], momentum=0.9, weight_decay=cfg.get('wd', 0.0))\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor if USE_CLASS_WEIGHTS else None)\n",
    "    \n",
    "    if USE_SAMPLER:\n",
    "        sample_weights = class_weights_tensor.cpu().numpy()[y_train]\n",
    "        train_sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=cfg['batch'], sampler=train_sampler)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=cfg['batch'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg['batch'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=cfg['batch'], shuffle=False)\n",
    "    \n",
    "    print(f\"Training batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 6\n",
    "    \n",
    "    for epoch in range(cfg['epochs']):\n",
    "        train_loss, train_acc = run_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_acc = run_epoch(model, val_loader, criterion, optimizer=None)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{cfg['epochs']} - Train loss {train_loss:.4f}, acc {train_acc:.4f} | Val loss {val_loss:.4f}, acc {val_acc:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Quick val prediction distribution\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_val_preds = []\n",
    "        for Xb, _ in val_loader:\n",
    "            Xb = Xb.to(device)\n",
    "            preds = model(Xb).argmax(dim=1).cpu().numpy()\n",
    "            all_val_preds.extend(preds)\n",
    "    from collections import Counter\n",
    "    pred_dist = Counter(all_val_preds)\n",
    "    print(f\"Val pred distribution: {pred_dist}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = run_epoch(model, test_loader, criterion, optimizer=None)\n",
    "    results.append({\n",
    "        'config': i,\n",
    "        'optimizer': cfg['opt'],\n",
    "        'lr': cfg['lr'],\n",
    "        'wd': cfg.get('wd', 0.0),\n",
    "        'batch_size': cfg['batch'],\n",
    "        'train_acc': train_acc,\n",
    "        'val_acc': val_acc,\n",
    "        'test_acc': test_acc\n",
    "    })\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Save model in both PyTorch (.pt) and HDF5 (.h5) formats\n",
    "    pt_path = os.path.join(models_dir, f'model_{i}.pt')\n",
    "    h5_path = os.path.join(models_dir, f'model_{i}.h5')\n",
    "    torch.save(model.state_dict(), pt_path)\n",
    "    save_model_as_h5(model, h5_path)\n",
    "    print(f\"Saved: {pt_path} and {h5_path}\")\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0054f0",
   "metadata": {},
   "source": [
    "# 12. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f42d3a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Results:\n",
      "   config optimizer      lr      wd  batch_size  train_acc   val_acc  test_acc\n",
      "0       1      Adam  0.0008  0.0001          64   0.836653  0.834049  0.830000\n",
      "1       2      Adam  0.0010  0.0001          64   0.828992  0.824034  0.794286\n",
      "2       3      Adam  0.0005  0.0001          64   0.831750  0.821173  0.802857\n",
      "3       4   RMSprop  0.0008  0.0000          64   0.844315  0.804006  0.787143\n",
      "4       5   RMSprop  0.0005  0.0000          64   0.831443  0.831187  0.817143\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(r'd:\\CodingRelated\\Codes.Ams\\ANNFINAL\\experiment_results_run2.csv', index=False)\n",
    "\n",
    "print(\"All Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5c3d41f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BEST MODEL\n",
      "============================================================\n",
      "Optimizer: Adam\n",
      "Learning Rate: 0.0008\n",
      "Test Accuracy: 83.00%\n",
      "\n",
      "✓ Meets 50% requirement!\n"
     ]
    }
   ],
   "source": [
    "# Best model\n",
    "best_idx = results_df['test_acc'].idxmax()\n",
    "best = results_df.iloc[best_idx]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BEST MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Optimizer: {best['optimizer']}\")\n",
    "print(f\"Learning Rate: {best['lr']}\")\n",
    "print(f\"Test Accuracy: {best['test_acc']:.2%}\")\n",
    "\n",
    "if best['test_acc'] >= 0.50:\n",
    "    print(\"\\n✓ Meets 50% requirement!\")\n",
    "else:\n",
    "    print(\"\\n✗ Below 50%\")\n",
    "\n",
    "best_model_path = os.path.join(models_dir, f\"model_{best_idx + 1}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5b2ff481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts directory: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\artifacts_run4\n"
     ]
    }
   ],
   "source": [
    "# Artifact paths for this run\n",
    "ARTIFACTS_DIR = r'd:\\CodingRelated\\Codes.Ams\\ANNFINAL\\artifacts_run4'\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "\n",
    "# Persist tokenizer and label encoder classes\n",
    "with open(os.path.join(ARTIFACTS_DIR, 'tokenizer_word_index.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(tokenizer.word_to_index, f)\n",
    "np.save(os.path.join(ARTIFACTS_DIR, 'label_classes.npy'), label_encoder.classes_)\n",
    "\n",
    "print(f\"Artifacts directory: {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb130d8",
   "metadata": {},
   "source": [
    "# 13. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "022d710b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model from: model_1.pt\n"
     ]
    }
   ],
   "source": [
    "# Load best model (match training embed_dim)\n",
    "best_model = LSTMClassifier(VOCAB_SIZE, embed_dim=200, num_classes=NUM_CLASSES).to(device)\n",
    "best_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "best_model.eval()\n",
    "\n",
    "# Get predictions\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.long).to(device)\n",
    "with torch.no_grad():\n",
    "    y_pred = best_model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = y_test\n",
    "\n",
    "print(f\"Loaded best model from: model_{best_idx + 1}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6fca778d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[68  0  0  0  0  1  0  1  0  0  0  0]\n",
      " [ 0 58  0  0  0  0  6  0  0  0  0  0]\n",
      " [ 0  0 51  0  0  1  0  1  0  3  0  0]\n",
      " [ 0  0  2 38  0  1  0  0  0  0  0  0]\n",
      " [ 1  0  0  0 45  0  0  0  0  0  0  0]\n",
      " [40  0  0  0  0 10  0  7  0  0  0  0]\n",
      " [ 0 44  0  0  0  0  4  0  0  0  0  1]\n",
      " [ 0  0  1  0  0  2  0 59  0  0  0  0]\n",
      " [ 0  0  1  1  0  0  0  0 63  1  0  0]\n",
      " [ 0  0  1  0  0  0  0  1  0 36  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 76  0]\n",
      " [ 0  0  0  0  0  0  1  0  0  1  0 73]]\n",
      "\n",
      "Saved confusion matrix to: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\artifacts_run4\\confusion_matrix.csv\n",
      "\n",
      "Accuracy per class:\n",
      "Agreement Date: 97.14%\n",
      "Agreement Date-Answer: 90.62%\n",
      "Anti-assignment: 91.07%\n",
      "Cap on Liability: 92.68%\n",
      "Document Name: 97.83%\n",
      "Effective Date: 17.54%\n",
      "Effective Date-Answer: 8.16%\n",
      "Expiration Date: 95.16%\n",
      "Governing Law: 95.45%\n",
      "License Grant: 94.74%\n",
      "Parties: 100.00%\n",
      "Parties-Answer: 97.33%\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix - save and print\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "cm_df = pd.DataFrame(cm, index=label_encoder.classes_, columns=label_encoder.classes_)\n",
    "\n",
    "cm_path = os.path.join(ARTIFACTS_DIR, 'confusion_matrix.csv')\n",
    "cm_df.to_csv(cm_path)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nSaved confusion matrix to: {cm_path}\")\n",
    "print(f\"\\nAccuracy per class:\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    class_acc = cm[i, i] / cm[i].sum() if cm[i].sum() > 0 else 0\n",
    "    print(f\"{class_name}: {class_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b5308458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "                       precision    recall  f1-score  support\n",
      "Agreement Date          0.623853  0.971429  0.759777    70.00\n",
      "Agreement Date-Answer   0.568627  0.906250  0.698795    64.00\n",
      "Anti-assignment         0.910714  0.910714  0.910714    56.00\n",
      "Cap on Liability        0.974359  0.926829  0.950000    41.00\n",
      "Document Name           1.000000  0.978261  0.989011    46.00\n",
      "Effective Date          0.666667  0.175439  0.277778    57.00\n",
      "Effective Date-Answer   0.363636  0.081633  0.133333    49.00\n",
      "Expiration Date         0.855072  0.951613  0.900763    62.00\n",
      "Governing Law           1.000000  0.954545  0.976744    66.00\n",
      "License Grant           0.878049  0.947368  0.911392    38.00\n",
      "Parties                 1.000000  1.000000  1.000000    76.00\n",
      "Parties-Answer          0.986486  0.973333  0.979866    75.00\n",
      "accuracy                0.830000  0.830000  0.830000     0.83\n",
      "macro avg               0.818955  0.814785  0.790681   700.00\n",
      "weighted avg            0.821708  0.830000  0.800220   700.00\n",
      "\n",
      "Saved classification report to: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\artifacts_run4\\classification_report.csv\n"
     ]
    }
   ],
   "source": [
    "# Classification report - save to artifacts\n",
    "print(\"\\nClassification Report:\")\n",
    "report = classification_report(\n",
    "    y_true_classes,\n",
    "    y_pred_classes,\n",
    "    target_names=label_encoder.classes_,\n",
    "    output_dict=True,\n",
    "    zero_division=0,\n",
    ")\n",
    "report_df = pd.DataFrame(report).T\n",
    "print(report_df)\n",
    "\n",
    "report_path = os.path.join(ARTIFACTS_DIR, 'classification_report.csv')\n",
    "report_df.to_csv(report_path)\n",
    "print(f\"\\nSaved classification report to: {report_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
