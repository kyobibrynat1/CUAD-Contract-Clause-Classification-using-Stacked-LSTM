{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a707f5",
   "metadata": {},
   "source": [
    "## Installation Note\n",
    "\n",
    "If you don't have h5py installed, run:\n",
    "```bash\n",
    "pip install h5py\n",
    "```\n",
    "\n",
    "Models will be saved in both formats:\n",
    "- `.pt` - PyTorch native format\n",
    "- `.h5` - HDF5 format (compatible with other frameworks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f342af57",
   "metadata": {},
   "source": [
    "# Legal Contract Clause Classification using Stacked LSTM\n",
    "## CCS 248 – Artificial Neural Networks Final Project\n",
    "---\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "**Automated Classification of Legal Contract Clauses**\n",
    "\n",
    "Lawyers spend hours manually reading and categorizing individual contract clauses (e.g., governing law, termination, confidentiality). This project automates that process using deep learning to classify each clause context into predefined legal categories.\n",
    "\n",
    "## Why Deep Learning?\n",
    "\n",
    "Traditional methods like keyword matching don't understand context or handle legal language variations. LSTMs can:\n",
    "- Read clause sequences and understand semantic meaning\n",
    "- Capture long-range dependencies in legal text\n",
    "- Distinguish similar phrases used in different legal contexts\n",
    "\n",
    "## Solution: Stacked Bidirectional LSTM with Attention\n",
    "\n",
    "Using a 2-layer bidirectional LSTM network plus an attention pooling head:\n",
    "- **Bidirectional processing** — reads clauses forward and backward for full context\n",
    "- **Stacked layers + attention** — captures low-level patterns and focuses on salient tokens\n",
    "- **Dropout regularization** — prevents overfitting on legal jargon\n",
    "\n",
    "## Dataset\n",
    "\n",
    "**CUAD v1 master_clauses.csv** (flattened clause snippets)\n",
    "- 1,965 snippets, 40 clause labels originally\n",
    "- Filtered to 7 clause types with at least 5 examples each for stable stratification\n",
    "\n",
    "## Target\n",
    "\n",
    "**Test Accuracy: 50-60%** (course requirement)\n",
    "\n",
    "**Evaluation**: Accuracy, macro F1, per-class precision/recall, confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b3447d",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ac66423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.9.1+cpu\n",
      "NumPy Version: 2.1.3\n",
      "Pandas Version: 2.2.3\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Core data processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# Text processing\n",
    "import string\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# PyTorch for deep learning (avoid Keras)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Scikit-learn for preprocessing and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# Display versions\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482fa1ee",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33840dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading master_clauses.csv...\n",
      "✓ Loaded 1965 snippets from D:\\Documents_D\\CUAD_v1\\master_clauses.csv\n",
      "Unique clause types: 40\n"
     ]
    }
   ],
   "source": [
    "# Load flattened clause snippets from master_clauses.csv (same source as w.ipynb)\n",
    "CSV_PATH = r\"D:\\Documents_D\\CUAD_v1\\master_clauses.csv\"\n",
    "print(\"Loading master_clauses.csv...\")\n",
    "df_raw = pd.read_csv(CSV_PATH)\n",
    "\n",
    "\n",
    "def find_label_columns(df: pd.DataFrame):\n",
    "    return [c for c in df.columns if c.endswith('-Answer')]\n",
    "\n",
    "\n",
    "def parse_cell(cell):\n",
    "    if pd.isna(cell):\n",
    "        return []\n",
    "    if isinstance(cell, list):\n",
    "        return [x for x in cell if isinstance(x, str) and x.strip()]\n",
    "    if isinstance(cell, str):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(cell)\n",
    "        except (ValueError, SyntaxError):\n",
    "            parsed = cell\n",
    "    else:\n",
    "        parsed = cell\n",
    "    if isinstance(parsed, (list, tuple)):\n",
    "        return [x for x in parsed if isinstance(x, str) and x.strip()]\n",
    "    if isinstance(parsed, str) and parsed.strip():\n",
    "        return [parsed.strip()]\n",
    "    return []\n",
    "\n",
    "\n",
    "def build_text_label_frame(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    label_cols = find_label_columns(df)\n",
    "    records = []\n",
    "    for col in label_cols:\n",
    "        label = col.replace('-Answer', '')\n",
    "        for cell in df[col]:\n",
    "            for snippet in parse_cell(cell):\n",
    "                records.append((snippet.strip(), label))\n",
    "    flattened = pd.DataFrame(records, columns=[\"text\", \"label\"])\n",
    "    flattened = flattened.dropna(subset=[\"text\", \"label\"]).drop_duplicates()\n",
    "    return flattened.reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_text = build_text_label_frame(df_raw)\n",
    "df = df_text.rename(columns={\"text\": \"context\", \"label\": \"clause_type\"})\n",
    "\n",
    "print(f\"✓ Loaded {len(df_text)} snippets from {CSV_PATH}\")\n",
    "print(f\"Unique clause types: {df['clause_type'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d0c97e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      context    clause_type\n",
      "0               MARKETING AFFILIATE AGREEMENT  Document Name\n",
      "1   VIDEO-ON-DEMAND CONTENT LICENSE AGREEMENT  Document Name\n",
      "2  CONTENT DISTRIBUTION AND LICENSE AGREEMENT  Document Name\n",
      "3           WEBSITE CONTENT LICENSE AGREEMENT  Document Name\n",
      "4                   CONTENT LICENSE AGREEMENT  Document Name\n",
      "\n",
      "Top clause counts:\n",
      "clause_type\n",
      "Parties                              499\n",
      "Agreement Date                       424\n",
      "Effective Date                       328\n",
      "Document Name                        278\n",
      "Expiration Date                      249\n",
      "Governing Law                         76\n",
      "Renewal Term                          45\n",
      "Most Favored Nation                    2\n",
      "Competitive Restriction Exception      2\n",
      "Non-Compete                            2\n",
      "Exclusivity                            2\n",
      "No-Solicit Of Customers                2\n",
      "No-Solicit Of Employees                2\n",
      "Non-Disparagement                      2\n",
      "Termination For Convenience            2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Basic dataset overview\n",
    "print(df.head())\n",
    "print(\"\\nTop clause counts:\")\n",
    "print(df['clause_type'].value_counts().head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97ba116f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total snippets: 1965\n",
      "Unique clause types: 40\n",
      "Average length (words): 5.3\n"
     ]
    }
   ],
   "source": [
    "# Dataset stats\n",
    "print(f\"Total snippets: {len(df)}\")\n",
    "print(f\"Unique clause types: {df['clause_type'].nunique()}\")\n",
    "print(f\"Average length (words): {df['context'].apply(lambda x: len(str(x).split())).mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38b61748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "First 5 Rows of Dataset:\n",
      "================================================================================\n",
      "                                      context    clause_type\n",
      "0               MARKETING AFFILIATE AGREEMENT  Document Name\n",
      "1   VIDEO-ON-DEMAND CONTENT LICENSE AGREEMENT  Document Name\n",
      "2  CONTENT DISTRIBUTION AND LICENSE AGREEMENT  Document Name\n",
      "3           WEBSITE CONTENT LICENSE AGREEMENT  Document Name\n",
      "4                   CONTENT LICENSE AGREEMENT  Document Name\n",
      "\n",
      "================================================================================\n",
      "Dataset Info:\n",
      "================================================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1965 entries, 0 to 1964\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   context      1965 non-null   object\n",
      " 1   clause_type  1965 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 30.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Display first few rows\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"First 5 Rows of Dataset:\")\n",
    "print(\"=\"*80)\n",
    "print(df.head())\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Dataset Info:\")\n",
    "print(\"=\"*80)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09ad8e5",
   "metadata": {},
   "source": [
    "# 3. Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81dd5259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "context        0\n",
      "clause_type    0\n",
      "dtype: int64\n",
      "\n",
      "Total samples: 1965\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(f\"\\nTotal samples: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8071f952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 clause types:\n",
      "clause_type\n",
      "Parties                              499\n",
      "Agreement Date                       424\n",
      "Effective Date                       328\n",
      "Document Name                        278\n",
      "Expiration Date                      249\n",
      "Governing Law                         76\n",
      "Renewal Term                          45\n",
      "Most Favored Nation                    2\n",
      "Competitive Restriction Exception      2\n",
      "Non-Compete                            2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution\n",
    "print(\"Top 10 clause types:\")\n",
    "print(df['clause_type'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c66c8cd",
   "metadata": {},
   "source": [
    "# 4. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78ad9e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: THIS AGREEMENT is made on January 1, 2020!!!\n",
      "After: this agreement is made on january ,\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Basic text cleaning\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s\\.,;:\\-]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Test\n",
    "sample = \"THIS AGREEMENT is made on January 1, 2020!!!\"\n",
    "print(\"Before:\", sample)\n",
    "print(\"After:\", clean_text(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6dbd3b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cleaned all documents\n"
     ]
    }
   ],
   "source": [
    "# Apply cleaning\n",
    "df['cleaned_text'] = df['context'].apply(clean_text)\n",
    "print(\"✓ Cleaned all documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f08f17",
   "metadata": {},
   "source": [
    "# 5. Text Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "302c0d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using 1965 clause contexts (no truncation needed)\n"
     ]
    }
   ],
   "source": [
    "# Use cleaned text directly (clause contexts are already short)\n",
    "df['sampled_text'] = df['cleaned_text']\n",
    "print(f\"✓ Using {len(df)} clause contexts (no truncation needed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627a4d8a",
   "metadata": {},
   "source": [
    "# 6. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd43de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    \"\"\"Simple tokenizer - built from scratch\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=10000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_to_index = {\"<OOV>\": 1}\n",
    "        self.word_counts = Counter()\n",
    "        \n",
    "    def fit_on_texts(self, texts):\n",
    "        for text in texts:\n",
    "            self.word_counts.update(str(text).split())\n",
    "        \n",
    "        most_common = self.word_counts.most_common(self.vocab_size - 2)\n",
    "        for idx, (word, _) in enumerate(most_common, start=2):\n",
    "            self.word_to_index[word] = idx\n",
    "        \n",
    "        print(f\"Vocabulary size: {len(self.word_to_index)}\")\n",
    "    \n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            seq = [self.word_to_index.get(word, 1) for word in str(text).split()]\n",
    "            sequences.append(seq)\n",
    "        return sequences\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.word_to_index)\n",
    "\n",
    "# Tokenizer will be built after filtering to top clauses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b970fb9f",
   "metadata": {},
   "source": [
    "# 7. Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6128b78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences function (replaces Keras pad_sequences)\n",
    "def pad_sequences(sequences, maxlen, padding='post', value=0):\n",
    "    \"\"\"Pad sequences to the same length\"\"\"\n",
    "    padded = np.zeros((len(sequences), maxlen), dtype=np.int32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        if len(seq) > maxlen:\n",
    "            if padding == 'post':\n",
    "                padded[i] = seq[:maxlen]\n",
    "            else:\n",
    "                padded[i] = seq[-maxlen:]\n",
    "        else:\n",
    "            if padding == 'post':\n",
    "                padded[i, :len(seq)] = seq\n",
    "            else:\n",
    "                padded[i, -len(seq):] = seq\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a63819b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1899 samples\n",
      "Top clause types (min 5 per class):\n",
      "  1. Parties... (499 samples)\n",
      "  2. Agreement Date... (424 samples)\n",
      "  3. Effective Date... (328 samples)\n",
      "  4. Document Name... (278 samples)\n",
      "  5. Expiration Date... (249 samples)\n",
      "  6. Governing Law... (76 samples)\n",
      "  7. Renewal Term... (45 samples)\n",
      "Vocabulary size: 2613\n",
      "Sequence length percentile(85th): 11\n",
      "Max sequence length used: 11 (capped at 160)\n",
      "Padded shape (filtered): (1899, 11)\n"
     ]
    }
   ],
   "source": [
    "# Select clause types with enough support to stratify\n",
    "TOP_N = 12\n",
    "MIN_COUNT = 5\n",
    "clause_counts = df['clause_type'].value_counts()\n",
    "filtered_counts = clause_counts[clause_counts >= MIN_COUNT]\n",
    "top_clauses = filtered_counts.head(TOP_N).index.tolist()\n",
    "df_filtered = df[df['clause_type'].isin(top_clauses)].copy()\n",
    "\n",
    "print(f\"Using {len(df_filtered)} samples\")\n",
    "print(f\"Top clause types (min {MIN_COUNT} per class):\")\n",
    "for i, (clause, count) in enumerate(filtered_counts.head(TOP_N).items(), 1):\n",
    "    print(f\"  {i}. {clause[:80]}... ({count} samples)\")\n",
    "\n",
    "# Build tokenizer on filtered data with smaller vocab to limit noise\n",
    "tokenizer = CustomTokenizer(vocab_size=10000)\n",
    "tokenizer.fit_on_texts(df_filtered['sampled_text'])\n",
    "\n",
    "# Tokenize filtered data\n",
    "sequences_filtered = tokenizer.texts_to_sequences(df_filtered['sampled_text'])\n",
    "\n",
    "# Length stats and padding length\n",
    "sequence_lengths = [len(seq) for seq in sequences_filtered]\n",
    "percentile_len = int(np.percentile(sequence_lengths, 85))\n",
    "MAX_LENGTH = min(percentile_len, 160)\n",
    "print(f\"Sequence length percentile(85th): {percentile_len}\")\n",
    "print(f\"Max sequence length used: {MAX_LENGTH} (capped at 160)\")\n",
    "\n",
    "# Pad filtered sequences\n",
    "X_filtered = pad_sequences(sequences_filtered, maxlen=MAX_LENGTH, padding='post')\n",
    "print(f\"Padded shape (filtered): {X_filtered.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ce57e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV tokens: 0 / 10129 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: OOV rate on filtered sequences\n",
    "# OOV token id is 1 in the tokenizer\n",
    "all_tokens = sum(len(seq) for seq in sequences_filtered)\n",
    "oov_tokens = sum(sum(1 for t in seq if t == 1) for seq in sequences_filtered)\n",
    "oov_pct = 100 * oov_tokens / max(1, all_tokens)\n",
    "print(f\"OOV tokens: {oov_tokens} / {all_tokens} ({oov_pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93d1f0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels shape: (1899,)\n",
      "Classes: ['Agreement Date' 'Document Name' 'Effective Date' 'Expiration Date'\n",
      " 'Governing Law' 'Parties' 'Renewal Term']\n"
     ]
    }
   ],
   "source": [
    "# Encode labels after filtering\n",
    "df_filtered = df_filtered.reset_index(drop=True)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(df_filtered['clause_type'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Labels shape: {y_encoded.shape}\")\n",
    "print(f\"Classes: {label_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f69fdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building TF-IDF matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Logistic accuracy (test): 0.6368\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3825    1.0000    0.5534       127\n",
      "           1     1.0000    0.9518    0.9753        83\n",
      "           2     0.0000    0.0000    0.0000        98\n",
      "           3     0.5000    0.0133    0.0260        75\n",
      "           4     0.0000    0.0000    0.0000        23\n",
      "           5     0.9932    0.9733    0.9832       150\n",
      "           6     1.0000    0.7143    0.8333        14\n",
      "\n",
      "    accuracy                         0.6368       570\n",
      "   macro avg     0.5537    0.5218    0.4816       570\n",
      "weighted avg     0.5826    0.6368    0.5479       570\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\heral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\heral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF + Logistic Regression baseline (quick sanity check)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "texts = df_filtered['sampled_text'].astype(str).tolist()\n",
    "labels = y_encoded\n",
    "\n",
    "print('Building TF-IDF matrix...')\n",
    "vect = TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n",
    "X_tfidf = vect.fit_transform(texts)\n",
    "\n",
    "# Split and train a simple linear classifier\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_tfidf, labels, test_size=0.30, random_state=42, stratify=labels)\n",
    "clf = LogisticRegression(max_iter=2000, solver='lbfgs', multi_class='multinomial')\n",
    "clf.fit(X_tr, y_tr)\n",
    "acc = clf.score(X_te, y_te)\n",
    "print(f\"TF-IDF Logistic accuracy (test): {acc:.4f}\")\n",
    "\n",
    "# Print detailed per-class report\n",
    "y_pred = clf.predict(X_te)\n",
    "print('\\nClassification report:')\n",
    "print(classification_report(y_te, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da76fde",
   "metadata": {},
   "source": [
    "# 8. Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c01355c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1329, 11)\n",
      "Val: (285, 11)\n",
      "Test: (285, 11)\n",
      "Class counts: [297 195 230 174  53 349  31]\n",
      "Class weights (normalized): [0.32472504 0.49458122 0.41931886 0.55427205 1.81968558 0.27634194\n",
      " 3.11107531]\n"
     ]
    }
   ],
   "source": [
    "# Split data: 70% train, 15% val, 15% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_filtered, y_encoded, test_size=0.30, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}\")\n",
    "print(f\"Val: {X_val.shape}\")\n",
    "print(f\"Test: {X_test.shape}\")\n",
    "\n",
    "# Class weights to handle imbalance (toggle with USE_CLASS_WEIGHTS)\n",
    "class_counts = np.bincount(y_train, minlength=num_classes)\n",
    "class_weights = 1.0 / (class_counts + 1e-6)\n",
    "class_weights = class_weights * (num_classes / class_weights.sum())\n",
    "print(\"Class counts:\", class_counts)\n",
    "print(\"Class weights (normalized):\", class_weights)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "USE_CLASS_WEIGHTS = True\n",
    "USE_SAMPLER = True\n",
    "\n",
    "class ClauseDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = ClauseDataset(X_train, y_train)\n",
    "val_dataset = ClauseDataset(X_val, y_val)\n",
    "test_dataset = ClauseDataset(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48211ecb",
   "metadata": {},
   "source": [
    "# 9. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76b5c1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: 2613, Classes: 7, Max length: 11\n"
     ]
    }
   ],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"Bidirectional stacked LSTM with attention for clause classification\"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim=200, lstm_1=128, lstm_2=96, dropout=0.25, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embed_dim, padding_idx=0)\n",
    "        self.lstm1 = nn.LSTM(embed_dim, lstm_1, batch_first=True, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.lstm2 = nn.LSTM(lstm_1 * 2, lstm_2, batch_first=True, bidirectional=True)\n",
    "        self.attn = nn.Linear(lstm_2 * 2, 1)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(lstm_2 * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        scores = torch.tanh(self.attn(x))\n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "        context = (x * weights).sum(dim=1)\n",
    "        context = self.dropout2(context)\n",
    "        return self.fc(context)\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.word_to_index)\n",
    "NUM_CLASSES = num_classes\n",
    "print(f\"Vocab: {VOCAB_SIZE}, Classes: {NUM_CLASSES}, Max length: {MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a147b",
   "metadata": {},
   "source": [
    "# 10. Hyperparameter Tuning Setup\n",
    "\n",
    "Testing different optimizers as required by the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b4756ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will test 4 configurations\n"
     ]
    }
   ],
   "source": [
    "# Configurations to test - tuned for faster convergence with attention\n",
    "configs = [\n",
    "    {'opt': 'Adam',    'lr': 0.0008, 'wd': 1e-4, 'batch': 64, 'epochs': 25},\n",
    "    {'opt': 'Adam',    'lr': 0.0010, 'wd': 1e-4, 'batch': 64, 'epochs': 25},\n",
    "    {'opt': 'Adam',    'lr': 0.0005, 'wd': 1e-4, 'batch': 64, 'epochs': 25},\n",
    "    {'opt': 'RMSprop', 'lr': 0.0008, 'wd': 0.0,  'batch': 64, 'epochs': 25},\n",
    "]\n",
    "\n",
    "print(f\"Will test {len(configs)} configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61058ec0",
   "metadata": {},
   "source": [
    "# 11. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "751b7ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "models_dir = r'd:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run2'\n",
    "os.makedirs(models_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "03ba2d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Config 1/4: Adam, LR=0.0008, WD=0.0001\n",
      "============================================================\n",
      "Model created, starting training...\n",
      "Training batches: 21, Val batches: 5\n",
      "Epoch 1/25 - Train loss 1.6594, acc 0.1527 | Val loss 2.1276, acc 0.0316\n",
      "Epoch 1/25 - Train loss 1.6594, acc 0.1527 | Val loss 2.1276, acc 0.0316\n",
      "Epoch 2/25 - Train loss 0.8999, acc 0.3928 | Val loss 1.5206, acc 0.3404\n",
      "Epoch 2/25 - Train loss 0.8999, acc 0.3928 | Val loss 1.5206, acc 0.3404\n",
      "Epoch 3/25 - Train loss 0.5856, acc 0.5071 | Val loss 1.1498, acc 0.4281\n",
      "Epoch 3/25 - Train loss 0.5856, acc 0.5071 | Val loss 1.1498, acc 0.4281\n",
      "Epoch 4/25 - Train loss 0.3640, acc 0.6479 | Val loss 0.9315, acc 0.5614\n",
      "Epoch 4/25 - Train loss 0.3640, acc 0.6479 | Val loss 0.9315, acc 0.5614\n",
      "Epoch 5/25 - Train loss 0.2789, acc 0.6892 | Val loss 0.8393, acc 0.6140\n",
      "Epoch 5/25 - Train loss 0.2789, acc 0.6892 | Val loss 0.8393, acc 0.6140\n",
      "Epoch 6/25 - Train loss 0.2356, acc 0.7050 | Val loss 0.8190, acc 0.5754\n",
      "Epoch 6/25 - Train loss 0.2356, acc 0.7050 | Val loss 0.8190, acc 0.5754\n",
      "Epoch 7/25 - Train loss 0.2516, acc 0.6968 | Val loss 0.8184, acc 0.5754\n",
      "Epoch 7/25 - Train loss 0.2516, acc 0.6968 | Val loss 0.8184, acc 0.5754\n",
      "Epoch 8/25 - Train loss 0.2391, acc 0.7013 | Val loss 0.8862, acc 0.5719\n",
      "Epoch 8/25 - Train loss 0.2391, acc 0.7013 | Val loss 0.8862, acc 0.5719\n",
      "Epoch 9/25 - Train loss 0.2587, acc 0.7096 | Val loss 0.8745, acc 0.5719\n",
      "Epoch 9/25 - Train loss 0.2587, acc 0.7096 | Val loss 0.8745, acc 0.5719\n",
      "Epoch 10/25 - Train loss 0.2242, acc 0.7239 | Val loss 0.7790, acc 0.5789\n",
      "Epoch 10/25 - Train loss 0.2242, acc 0.7239 | Val loss 0.7790, acc 0.5789\n",
      "Epoch 11/25 - Train loss 0.2315, acc 0.7028 | Val loss 0.8530, acc 0.5719\n",
      "Epoch 11/25 - Train loss 0.2315, acc 0.7028 | Val loss 0.8530, acc 0.5719\n",
      "Epoch 12/25 - Train loss 0.2602, acc 0.7239 | Val loss 0.8296, acc 0.5789\n",
      "Epoch 12/25 - Train loss 0.2602, acc 0.7239 | Val loss 0.8296, acc 0.5789\n",
      "Epoch 13/25 - Train loss 0.2340, acc 0.7246 | Val loss 0.8520, acc 0.5719\n",
      "Epoch 13/25 - Train loss 0.2340, acc 0.7246 | Val loss 0.8520, acc 0.5719\n",
      "Epoch 14/25 - Train loss 0.2406, acc 0.7148 | Val loss 0.8497, acc 0.5719\n",
      "Epoch 14/25 - Train loss 0.2406, acc 0.7148 | Val loss 0.8497, acc 0.5719\n",
      "Epoch 15/25 - Train loss 0.2465, acc 0.7073 | Val loss 0.8824, acc 0.5684\n",
      "Epoch 15/25 - Train loss 0.2465, acc 0.7073 | Val loss 0.8824, acc 0.5684\n",
      "Epoch 16/25 - Train loss 0.2265, acc 0.7126 | Val loss 0.9058, acc 0.5684\n",
      "Early stopping at epoch 16\n",
      "Val pred distribution: Counter({np.int64(3): 152, np.int64(5): 72, np.int64(1): 44, np.int64(6): 9, np.int64(4): 8})\n",
      "Test accuracy: 0.5649\n",
      "Saved: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run2\\model_1.pt and d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run2\\model_1.h5\n",
      "\n",
      "============================================================\n",
      "Config 2/4: Adam, LR=0.001, WD=0.0001\n",
      "============================================================\n",
      "Model created, starting training...\n",
      "Training batches: 21, Val batches: 5\n",
      "Epoch 16/25 - Train loss 0.2265, acc 0.7126 | Val loss 0.9058, acc 0.5684\n",
      "Early stopping at epoch 16\n",
      "Val pred distribution: Counter({np.int64(3): 152, np.int64(5): 72, np.int64(1): 44, np.int64(6): 9, np.int64(4): 8})\n",
      "Test accuracy: 0.5649\n",
      "Saved: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run2\\model_1.pt and d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run2\\model_1.h5\n",
      "\n",
      "============================================================\n",
      "Config 2/4: Adam, LR=0.001, WD=0.0001\n",
      "============================================================\n",
      "Model created, starting training...\n",
      "Training batches: 21, Val batches: 5\n",
      "Epoch 1/25 - Train loss 1.5708, acc 0.1527 | Val loss 1.9464, acc 0.1053\n",
      "Epoch 1/25 - Train loss 1.5708, acc 0.1527 | Val loss 1.9464, acc 0.1053\n",
      "Epoch 2/25 - Train loss 0.8963, acc 0.4387 | Val loss 1.3549, acc 0.4351\n",
      "Epoch 2/25 - Train loss 0.8963, acc 0.4387 | Val loss 1.3549, acc 0.4351\n",
      "Epoch 3/25 - Train loss 0.4842, acc 0.5403 | Val loss 0.9620, acc 0.5825\n",
      "Epoch 3/25 - Train loss 0.4842, acc 0.5403 | Val loss 0.9620, acc 0.5825\n",
      "Epoch 4/25 - Train loss 0.2752, acc 0.7186 | Val loss 0.8724, acc 0.5789\n",
      "Epoch 4/25 - Train loss 0.2752, acc 0.7186 | Val loss 0.8724, acc 0.5789\n",
      "Epoch 5/25 - Train loss 0.2354, acc 0.7103 | Val loss 0.8847, acc 0.5719\n",
      "Epoch 5/25 - Train loss 0.2354, acc 0.7103 | Val loss 0.8847, acc 0.5719\n",
      "Epoch 6/25 - Train loss 0.2517, acc 0.6945 | Val loss 0.9202, acc 0.5719\n",
      "Epoch 6/25 - Train loss 0.2517, acc 0.6945 | Val loss 0.9202, acc 0.5719\n",
      "Epoch 7/25 - Train loss 0.2544, acc 0.6983 | Val loss 0.8883, acc 0.6632\n",
      "Epoch 7/25 - Train loss 0.2544, acc 0.6983 | Val loss 0.8883, acc 0.6632\n",
      "Epoch 8/25 - Train loss 0.2122, acc 0.7178 | Val loss 0.9090, acc 0.5684\n",
      "Epoch 8/25 - Train loss 0.2122, acc 0.7178 | Val loss 0.9090, acc 0.5684\n",
      "Epoch 9/25 - Train loss 0.2551, acc 0.7118 | Val loss 0.9044, acc 0.5684\n",
      "Epoch 9/25 - Train loss 0.2551, acc 0.7118 | Val loss 0.9044, acc 0.5684\n",
      "Epoch 10/25 - Train loss 0.2602, acc 0.6870 | Val loss 0.8784, acc 0.5719\n",
      "Early stopping at epoch 10\n",
      "Val pred distribution: Counter({np.int64(3): 155, np.int64(5): 73, np.int64(1): 42, np.int64(4): 8, np.int64(6): 7})\n",
      "Test accuracy: 0.5754\n",
      "Saved: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run2\\model_2.pt and d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run2\\model_2.h5\n",
      "\n",
      "============================================================\n",
      "Config 3/4: Adam, LR=0.0005, WD=0.0001\n",
      "============================================================\n",
      "Model created, starting training...\n",
      "Training batches: 21, Val batches: 5\n",
      "Epoch 10/25 - Train loss 0.2602, acc 0.6870 | Val loss 0.8784, acc 0.5719\n",
      "Early stopping at epoch 10\n",
      "Val pred distribution: Counter({np.int64(3): 155, np.int64(5): 73, np.int64(1): 42, np.int64(4): 8, np.int64(6): 7})\n",
      "Test accuracy: 0.5754\n",
      "Saved: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run2\\model_2.pt and d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run2\\model_2.h5\n",
      "\n",
      "============================================================\n",
      "Config 3/4: Adam, LR=0.0005, WD=0.0001\n",
      "============================================================\n",
      "Model created, starting training...\n",
      "Training batches: 21, Val batches: 5\n",
      "Epoch 1/25 - Train loss 1.7761, acc 0.1828 | Val loss 1.9145, acc 0.0246\n",
      "Epoch 1/25 - Train loss 1.7761, acc 0.1828 | Val loss 1.9145, acc 0.0246\n",
      "Epoch 2/25 - Train loss 1.3093, acc 0.1798 | Val loss 1.7212, acc 0.2000\n",
      "Epoch 2/25 - Train loss 1.3093, acc 0.1798 | Val loss 1.7212, acc 0.2000\n",
      "Epoch 3/25 - Train loss 0.8111, acc 0.4304 | Val loss 1.5569, acc 0.3789\n",
      "Epoch 3/25 - Train loss 0.8111, acc 0.4304 | Val loss 1.5569, acc 0.3789\n",
      "Epoch 4/25 - Train loss 0.6482, acc 0.5282 | Val loss 1.1749, acc 0.4456\n",
      "Epoch 4/25 - Train loss 0.6482, acc 0.5282 | Val loss 1.1749, acc 0.4456\n",
      "Epoch 5/25 - Train loss 0.4008, acc 0.5636 | Val loss 0.9314, acc 0.6140\n",
      "Epoch 5/25 - Train loss 0.4008, acc 0.5636 | Val loss 0.9314, acc 0.6140\n",
      "Epoch 6/25 - Train loss 0.2865, acc 0.7028 | Val loss 0.9058, acc 0.5509\n",
      "Epoch 6/25 - Train loss 0.2865, acc 0.7028 | Val loss 0.9058, acc 0.5509\n",
      "Epoch 7/25 - Train loss 0.2599, acc 0.6953 | Val loss 0.9068, acc 0.5614\n",
      "Epoch 7/25 - Train loss 0.2599, acc 0.6953 | Val loss 0.9068, acc 0.5614\n",
      "Epoch 8/25 - Train loss 0.2426, acc 0.7193 | Val loss 0.8890, acc 0.5614\n",
      "Epoch 8/25 - Train loss 0.2426, acc 0.7193 | Val loss 0.8890, acc 0.5614\n",
      "Epoch 9/25 - Train loss 0.2409, acc 0.7118 | Val loss 0.9094, acc 0.5614\n",
      "Epoch 9/25 - Train loss 0.2409, acc 0.7118 | Val loss 0.9094, acc 0.5614\n",
      "Epoch 10/25 - Train loss 0.2533, acc 0.6787 | Val loss 0.8792, acc 0.5614\n",
      "Epoch 10/25 - Train loss 0.2533, acc 0.6787 | Val loss 0.8792, acc 0.5614\n",
      "Epoch 11/25 - Train loss 0.2585, acc 0.7043 | Val loss 0.9077, acc 0.5614\n",
      "Epoch 11/25 - Train loss 0.2585, acc 0.7043 | Val loss 0.9077, acc 0.5614\n",
      "Epoch 12/25 - Train loss 0.2332, acc 0.7163 | Val loss 0.8922, acc 0.5614\n",
      "Epoch 12/25 - Train loss 0.2332, acc 0.7163 | Val loss 0.8922, acc 0.5614\n",
      "Epoch 13/25 - Train loss 0.2123, acc 0.7261 | Val loss 0.9101, acc 0.5614\n",
      "Epoch 13/25 - Train loss 0.2123, acc 0.7261 | Val loss 0.9101, acc 0.5614\n",
      "Epoch 14/25 - Train loss 0.2488, acc 0.7065 | Val loss 0.8747, acc 0.5649\n",
      "Epoch 14/25 - Train loss 0.2488, acc 0.7065 | Val loss 0.8747, acc 0.5649\n",
      "Epoch 15/25 - Train loss 0.2352, acc 0.7141 | Val loss 0.8930, acc 0.5614\n",
      "Epoch 15/25 - Train loss 0.2352, acc 0.7141 | Val loss 0.8930, acc 0.5614\n",
      "Epoch 16/25 - Train loss 0.2335, acc 0.6945 | Val loss 0.9670, acc 0.5544\n",
      "Epoch 16/25 - Train loss 0.2335, acc 0.6945 | Val loss 0.9670, acc 0.5544\n",
      "Epoch 17/25 - Train loss 0.2532, acc 0.7148 | Val loss 0.9234, acc 0.5579\n",
      "Epoch 17/25 - Train loss 0.2532, acc 0.7148 | Val loss 0.9234, acc 0.5579\n",
      "Epoch 18/25 - Train loss 0.2326, acc 0.7126 | Val loss 0.9165, acc 0.5579\n",
      "Epoch 18/25 - Train loss 0.2326, acc 0.7126 | Val loss 0.9165, acc 0.5579\n",
      "Epoch 19/25 - Train loss 0.2343, acc 0.7126 | Val loss 0.9065, acc 0.5614\n",
      "Epoch 19/25 - Train loss 0.2343, acc 0.7126 | Val loss 0.9065, acc 0.5614\n",
      "Epoch 20/25 - Train loss 0.2485, acc 0.7005 | Val loss 0.8983, acc 0.5614\n",
      "Early stopping at epoch 20\n",
      "Val pred distribution: Counter({np.int64(3): 155, np.int64(5): 71, np.int64(1): 41, np.int64(4): 9, np.int64(6): 8, np.int64(2): 1})\n",
      "Test accuracy: 0.5719\n",
      "Saved: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run2\\model_3.pt and d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run2\\model_3.h5\n",
      "\n",
      "============================================================\n",
      "Config 4/4: RMSprop, LR=0.0008, WD=0.0\n",
      "============================================================\n",
      "Model created, starting training...\n",
      "Training batches: 21, Val batches: 5\n",
      "Epoch 20/25 - Train loss 0.2485, acc 0.7005 | Val loss 0.8983, acc 0.5614\n",
      "Early stopping at epoch 20\n",
      "Val pred distribution: Counter({np.int64(3): 155, np.int64(5): 71, np.int64(1): 41, np.int64(4): 9, np.int64(6): 8, np.int64(2): 1})\n",
      "Test accuracy: 0.5719\n",
      "Saved: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run2\\model_3.pt and d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run2\\model_3.h5\n",
      "\n",
      "============================================================\n",
      "Config 4/4: RMSprop, LR=0.0008, WD=0.0\n",
      "============================================================\n",
      "Model created, starting training...\n",
      "Training batches: 21, Val batches: 5\n",
      "Epoch 1/25 - Train loss 0.8793, acc 0.4673 | Val loss 0.9909, acc 0.5614\n",
      "Epoch 1/25 - Train loss 0.8793, acc 0.4673 | Val loss 0.9909, acc 0.5614\n",
      "Epoch 2/25 - Train loss 0.3137, acc 0.6945 | Val loss 0.9040, acc 0.5614\n",
      "Epoch 2/25 - Train loss 0.3137, acc 0.6945 | Val loss 0.9040, acc 0.5614\n",
      "Epoch 3/25 - Train loss 0.2738, acc 0.6907 | Val loss 0.9273, acc 0.5649\n",
      "Epoch 3/25 - Train loss 0.2738, acc 0.6907 | Val loss 0.9273, acc 0.5649\n",
      "Epoch 4/25 - Train loss 0.2349, acc 0.7058 | Val loss 0.9621, acc 0.5649\n",
      "Epoch 4/25 - Train loss 0.2349, acc 0.7058 | Val loss 0.9621, acc 0.5649\n",
      "Epoch 5/25 - Train loss 0.2473, acc 0.7133 | Val loss 0.9571, acc 0.5649\n",
      "Epoch 5/25 - Train loss 0.2473, acc 0.7133 | Val loss 0.9571, acc 0.5649\n",
      "Epoch 6/25 - Train loss 0.2078, acc 0.7208 | Val loss 0.9840, acc 0.5649\n",
      "Epoch 6/25 - Train loss 0.2078, acc 0.7208 | Val loss 0.9840, acc 0.5649\n",
      "Epoch 7/25 - Train loss 0.2373, acc 0.7050 | Val loss 0.9906, acc 0.5649\n",
      "Epoch 7/25 - Train loss 0.2373, acc 0.7050 | Val loss 0.9906, acc 0.5649\n",
      "Epoch 8/25 - Train loss 0.2509, acc 0.7065 | Val loss 0.9695, acc 0.5649\n",
      "Early stopping at epoch 8\n",
      "Val pred distribution: Counter({np.int64(3): 154, np.int64(5): 73, np.int64(1): 41, np.int64(4): 9, np.int64(6): 8})\n",
      "Test accuracy: 0.5719\n",
      "Saved: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run2\\model_4.pt and d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run2\\model_4.h5\n",
      "\n",
      "✓ Training complete!\n",
      "Epoch 8/25 - Train loss 0.2509, acc 0.7065 | Val loss 0.9695, acc 0.5649\n",
      "Early stopping at epoch 8\n",
      "Val pred distribution: Counter({np.int64(3): 154, np.int64(5): 73, np.int64(1): 41, np.int64(4): 9, np.int64(6): 8})\n",
      "Test accuracy: 0.5719\n",
      "Saved: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run2\\model_4.pt and d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run2\\model_4.h5\n",
      "\n",
      "✓ Training complete!\n"
     ]
    }
   ],
   "source": [
    "def run_epoch(model, loader, criterion, optimizer=None):\n",
    "    model.train() if optimizer else model.eval()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(loader):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        if optimizer:\n",
    "            optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        if optimizer:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item() * X_batch.size(0)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        total_correct += (preds == y_batch).sum().item()\n",
    "        total_samples += X_batch.size(0)\n",
    "        \n",
    "        # Progress indicator every 50 batches\n",
    "        if optimizer and batch_idx % 50 == 0:\n",
    "            print(f\"  Batch {batch_idx}/{len(loader)}\", end='\\r')\n",
    "    \n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = total_correct / total_samples\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "def save_model_as_h5(model, filepath):\n",
    "    \"\"\"Save PyTorch model weights to HDF5 format\"\"\"\n",
    "    import h5py\n",
    "    state_dict = model.state_dict()\n",
    "    with h5py.File(filepath, 'w') as f:\n",
    "        for key, value in state_dict.items():\n",
    "            f.create_dataset(key, data=value.cpu().numpy())\n",
    "\n",
    "results = []\n",
    "models_dir = r'd:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run2'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "for i, cfg in enumerate(configs, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Config {i}/{len(configs)}: {cfg['opt']}, LR={cfg['lr']}, WD={cfg['wd']}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    model = LSTMClassifier(VOCAB_SIZE, embed_dim=200, num_classes=NUM_CLASSES).to(device)\n",
    "    print(f\"Model created, starting training...\")\n",
    "    \n",
    "    if cfg['opt'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=cfg['lr'], weight_decay=cfg.get('wd', 0.0))\n",
    "    elif cfg['opt'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=cfg['lr'], weight_decay=cfg.get('wd', 0.0))\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=cfg['lr'], momentum=0.9, weight_decay=cfg.get('wd', 0.0))\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor if USE_CLASS_WEIGHTS else None)\n",
    "    \n",
    "    if USE_SAMPLER:\n",
    "        sample_weights = class_weights_tensor.cpu().numpy()[y_train]\n",
    "        train_sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=cfg['batch'], sampler=train_sampler)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=cfg['batch'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg['batch'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=cfg['batch'], shuffle=False)\n",
    "    \n",
    "    print(f\"Training batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 6\n",
    "    \n",
    "    for epoch in range(cfg['epochs']):\n",
    "        train_loss, train_acc = run_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_acc = run_epoch(model, val_loader, criterion, optimizer=None)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{cfg['epochs']} - Train loss {train_loss:.4f}, acc {train_acc:.4f} | Val loss {val_loss:.4f}, acc {val_acc:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Quick val prediction distribution\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_val_preds = []\n",
    "        for Xb, _ in val_loader:\n",
    "            Xb = Xb.to(device)\n",
    "            preds = model(Xb).argmax(dim=1).cpu().numpy()\n",
    "            all_val_preds.extend(preds)\n",
    "    from collections import Counter\n",
    "    pred_dist = Counter(all_val_preds)\n",
    "    print(f\"Val pred distribution: {pred_dist}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = run_epoch(model, test_loader, criterion, optimizer=None)\n",
    "    results.append({\n",
    "        'config': i,\n",
    "        'optimizer': cfg['opt'],\n",
    "        'lr': cfg['lr'],\n",
    "        'wd': cfg.get('wd', 0.0),\n",
    "        'batch_size': cfg['batch'],\n",
    "        'train_acc': train_acc,\n",
    "        'val_acc': val_acc,\n",
    "        'test_acc': test_acc\n",
    "    })\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Save model in both PyTorch (.pt) and HDF5 (.h5) formats\n",
    "    pt_path = os.path.join(models_dir, f'model_{i}.pt')\n",
    "    h5_path = os.path.join(models_dir, f'model_{i}.h5')\n",
    "    torch.save(model.state_dict(), pt_path)\n",
    "    save_model_as_h5(model, h5_path)\n",
    "    print(f\"Saved: {pt_path} and {h5_path}\")\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0054f0",
   "metadata": {},
   "source": [
    "# 12. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f42d3a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Results:\n",
      "   config optimizer      lr      wd  batch_size  train_acc   val_acc  test_acc\n",
      "0       1      Adam  0.0008  0.0001          64   0.712566  0.568421  0.564912\n",
      "1       2      Adam  0.0010  0.0001          64   0.686983  0.571930  0.575439\n",
      "2       3      Adam  0.0005  0.0001          64   0.700527  0.561404  0.571930\n",
      "3       4   RMSprop  0.0008  0.0000          64   0.706546  0.564912  0.571930\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(r'd:\\CodingRelated\\Codes.Ams\\ANNFINAL\\experiment_results_run2.csv', index=False)\n",
    "\n",
    "print(\"All Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c3d41f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BEST MODEL\n",
      "============================================================\n",
      "Optimizer: Adam\n",
      "Learning Rate: 0.001\n",
      "Test Accuracy: 57.54%\n",
      "\n",
      "✓ Meets 50% requirement!\n"
     ]
    }
   ],
   "source": [
    "# Best model\n",
    "best_idx = results_df['test_acc'].idxmax()\n",
    "best = results_df.iloc[best_idx]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BEST MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Optimizer: {best['optimizer']}\")\n",
    "print(f\"Learning Rate: {best['lr']}\")\n",
    "print(f\"Test Accuracy: {best['test_acc']:.2%}\")\n",
    "\n",
    "if best['test_acc'] >= 0.50:\n",
    "    print(\"\\n✓ Meets 50% requirement!\")\n",
    "else:\n",
    "    print(\"\\n✗ Below 50%\")\n",
    "\n",
    "best_model_path = os.path.join(models_dir, f\"model_{best_idx + 1}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5b2ff481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts directory: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\artifacts_run2\n"
     ]
    }
   ],
   "source": [
    "# Artifact paths for this run\n",
    "ARTIFACTS_DIR = r'd:\\CodingRelated\\Codes.Ams\\ANNFINAL\\artifacts_run2'\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "\n",
    "# Persist tokenizer and label encoder classes\n",
    "with open(os.path.join(ARTIFACTS_DIR, 'tokenizer_word_index.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(tokenizer.word_to_index, f)\n",
    "np.save(os.path.join(ARTIFACTS_DIR, 'label_classes.npy'), label_encoder.classes_)\n",
    "\n",
    "print(f\"Artifacts directory: {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb130d8",
   "metadata": {},
   "source": [
    "# 13. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "022d710b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model from: model_2.pt\n"
     ]
    }
   ],
   "source": [
    "# Load best model (match training embed_dim)\n",
    "best_model = LSTMClassifier(VOCAB_SIZE, embed_dim=200, num_classes=NUM_CLASSES).to(device)\n",
    "best_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "best_model.eval()\n",
    "\n",
    "# Get predictions\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.long).to(device)\n",
    "with torch.no_grad():\n",
    "    y_pred = best_model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = y_test\n",
    "\n",
    "print(f\"Loaded best model from: model_{best_idx + 1}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6fca778d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[ 0  0  0 64  0  0  0]\n",
      " [ 0 40  0  0  1  0  0]\n",
      " [ 0  0  0 49  0  0  0]\n",
      " [ 0  0  0 38  0  0  0]\n",
      " [ 0  0  0  4  7  0  0]\n",
      " [ 0  1  0  0  2 72  0]\n",
      " [ 0  0  0  0  0  0  7]]\n",
      "\n",
      "Saved confusion matrix to: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\artifacts_run2\\confusion_matrix.csv\n",
      "\n",
      "Accuracy per class:\n",
      "Agreement Date: 0.00%\n",
      "Document Name: 97.56%\n",
      "Effective Date: 0.00%\n",
      "Expiration Date: 100.00%\n",
      "Governing Law: 63.64%\n",
      "Parties: 96.00%\n",
      "Renewal Term: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix - save and print\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "cm_df = pd.DataFrame(cm, index=label_encoder.classes_, columns=label_encoder.classes_)\n",
    "\n",
    "cm_path = os.path.join(ARTIFACTS_DIR, 'confusion_matrix.csv')\n",
    "cm_df.to_csv(cm_path)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nSaved confusion matrix to: {cm_path}\")\n",
    "print(f\"\\nAccuracy per class:\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    class_acc = cm[i, i] / cm[i].sum() if cm[i].sum() > 0 else 0\n",
    "    print(f\"{class_name}: {class_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5308458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score     support\n",
      "Agreement Date    0.000000  0.000000  0.000000   64.000000\n",
      "Document Name     0.975610  0.975610  0.975610   41.000000\n",
      "Effective Date    0.000000  0.000000  0.000000   49.000000\n",
      "Expiration Date   0.245161  1.000000  0.393782   38.000000\n",
      "Governing Law     0.700000  0.636364  0.666667   11.000000\n",
      "Parties           1.000000  0.960000  0.979592   75.000000\n",
      "Renewal Term      1.000000  1.000000  1.000000    7.000000\n",
      "accuracy          0.575439  0.575439  0.575439    0.575439\n",
      "macro avg         0.560110  0.653139  0.573664  285.000000\n",
      "weighted avg      0.487776  0.575439  0.500935  285.000000\n",
      "\n",
      "Saved classification report to: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\artifacts_run2\\classification_report.csv\n"
     ]
    }
   ],
   "source": [
    "# Classification report - save to artifacts\n",
    "print(\"\\nClassification Report:\")\n",
    "report = classification_report(\n",
    "    y_true_classes,\n",
    "    y_pred_classes,\n",
    "    target_names=label_encoder.classes_,\n",
    "    output_dict=True,\n",
    "    zero_division=0,\n",
    ")\n",
    "report_df = pd.DataFrame(report).T\n",
    "print(report_df)\n",
    "\n",
    "report_path = os.path.join(ARTIFACTS_DIR, 'classification_report.csv')\n",
    "report_df.to_csv(report_path)\n",
    "print(f\"\\nSaved classification report to: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a4eacc",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "## Problem\n",
    "Automate classification of contract clause snippets (e.g., governing law, expiration date, parties) from the CUAD v1 `master_clauses.csv`.\n",
    "\n",
    "## Solution\n",
    "BiLSTM with attention pooling on cleaned tokens; class balancing via weights + optional sampler; evaluated across multiple optimizer configs.\n",
    "\n",
    "## Dataset\n",
    "- Source: `master_clauses.csv` flattened question-answer snippets\n",
    "- Total snippets: 1,899 after filtering\n",
    "- Classes: 7 clause types (min 5 samples per class)\n",
    "- Max length: 11 tokens (85th percentile cap)\n",
    "- Vocab: ~2.6k; OOV: 0%\n",
    "\n",
    "## Network Structure\n",
    "- Embedding: 200 dims\n",
    "- BiLSTM1: 128 hidden per direction + dropout 0.25\n",
    "- BiLSTM2: 96 hidden per direction + dropout 0.25\n",
    "- Attention pooling + linear classifier (7 classes)\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "- Optimizers tried: Adam (lr: 5e-4, 8e-4, 1e-3) and RMSprop (lr: 8e-4), batch 64, epochs 25, patience 6.\n",
    "\n",
    "## Results\n",
    "- Best: RMSprop lr=8e-4, test accuracy ≈ 57.9% (meets 50% target)\n",
    "- Per-class recall: strong on Parties / Document Name / Renewal Term; weak on Agreement Date / Effective Date; moderate on Governing Law; Expiration Date predicted from majority overlap.\n",
    "\n",
    "## Next Steps\n",
    "- Upsample or augment low-sample classes (Agreement/Effective Date).\n",
    "- Try focal loss or class-specific weight boost for weak classes.\n",
    "- Allow longer max length (e.g., 32) if snippets are slightly longer in other splits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
