{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a707f5",
   "metadata": {},
   "source": [
    "## Installation Note\n",
    "\n",
    "If you don't have h5py installed, run:\n",
    "```bash\n",
    "pip install h5py\n",
    "```\n",
    "\n",
    "Models will be saved in both formats:\n",
    "- `.pt` - PyTorch native format\n",
    "- `.h5` - HDF5 format (compatible with other frameworks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f342af57",
   "metadata": {},
   "source": [
    "# Legal Contract Clause Classification using Stacked LSTM\n",
    "## CCS 248 – Artificial Neural Networks Final Project\n",
    "---\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "**Automated Classification of Legal Contract Clauses**\n",
    "\n",
    "Lawyers spend hours manually reading and categorizing individual contract clauses (e.g., governing law, termination, confidentiality). This project automates that process using deep learning to classify each clause context into predefined legal categories.\n",
    "\n",
    "## Why Deep Learning?\n",
    "\n",
    "Traditional methods like keyword matching don't understand context or handle legal language variations. LSTMs can:\n",
    "- Read clause sequences and understand semantic meaning\n",
    "- Capture long-range dependencies in legal text\n",
    "- Distinguish similar phrases used in different legal contexts\n",
    "\n",
    "## Solution: Stacked Bidirectional LSTM\n",
    "\n",
    "Using a 2-layer bidirectional LSTM network:\n",
    "- **Bidirectional processing** — reads clauses forward and backward for full context\n",
    "- **Stacked layers** — captures both low-level patterns (legal terms) and high-level structure\n",
    "- **Dropout regularization** — prevents overfitting on legal jargon\n",
    "\n",
    "## Dataset\n",
    "\n",
    "**CUAD v1** - Contract Understanding Atticus Dataset\n",
    "- 510 commercial legal contracts\n",
    "- **~13,000 labeled clause contexts** (our training samples)\n",
    "- 41 different clause types (attorney-annotated)\n",
    "\n",
    "Using the **top 10 most common clause types** for this project.\n",
    "\n",
    "## Target\n",
    "\n",
    "**Test Accuracy: 50-60%** (as required by the course)\n",
    "\n",
    "**Evaluation**: Accuracy, macro F1, per-class precision/recall, confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b3447d",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ac66423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.9.1+cpu\n",
      "NumPy Version: 2.1.3\n",
      "Pandas Version: 2.2.3\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Core data processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# Text processing\n",
    "import string\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# PyTorch for deep learning (avoid Keras)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Scikit-learn for preprocessing and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# Display versions\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482fa1ee",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33840dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CUAD v1 dataset...\n",
      "✓ Dataset loaded successfully!\n",
      "Dataset type: <class 'dict'>\n",
      "\n",
      "Top-level keys: ['version', 'data']\n"
     ]
    }
   ],
   "source": [
    "# Define dataset path\n",
    "DATASET_PATH = r\"d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\CUAD_v1\\CUAD_v1.json\"\n",
    "TEXT_FOLDER = r\"d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\CUAD_v1\\full_contract_txt\"\n",
    "\n",
    "# Load the CUAD JSON dataset\n",
    "print(\"Loading CUAD v1 dataset...\")\n",
    "with open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
    "    cuad_data = json.load(f)\n",
    "\n",
    "print(f\"✓ Dataset loaded successfully!\")\n",
    "print(f\"Dataset type: {type(cuad_data)}\")\n",
    "print(f\"\\nTop-level keys: {list(cuad_data.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d0c97e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 510\n",
      "\n",
      "================================================================================\n",
      "Sample Document Structure:\n",
      "================================================================================\n",
      "Title: LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGREEMENT\n",
      "\n",
      "Keys in document: ['title', 'paragraphs']\n",
      "Number of paragraphs: 1\n",
      "\n",
      "First paragraph keys: ['qas', 'context']\n"
     ]
    }
   ],
   "source": [
    "# Explore the data structure\n",
    "data_entries = cuad_data['data']\n",
    "print(f\"Number of documents: {len(data_entries)}\")\n",
    "\n",
    "# Display first document structure\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample Document Structure:\")\n",
    "print(\"=\"*80)\n",
    "first_doc = data_entries[0]\n",
    "print(f\"Title: {first_doc.get('title', 'N/A')}\")\n",
    "print(f\"\\nKeys in document: {list(first_doc.keys())}\")\n",
    "\n",
    "if 'paragraphs' in first_doc:\n",
    "    print(f\"Number of paragraphs: {len(first_doc['paragraphs'])}\")\n",
    "    if len(first_doc['paragraphs']) > 0:\n",
    "        print(f\"\\nFirst paragraph keys: {list(first_doc['paragraphs'][0].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97ba116f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created DataFrame with 6693 clause samples\n",
      "\n",
      "DataFrame Shape: (6693, 3)\n",
      "\n",
      "Column Names:\n",
      "['document_title', 'context', 'clause_type']\n",
      "\n",
      "Unique contracts: 509\n",
      "Unique clause types: 41\n",
      "\n",
      "Unique contracts: 509\n",
      "Unique clause types: 41\n"
     ]
    }
   ],
   "source": [
    "# Extract data - collect multiple clause types but avoid duplicates\n",
    "documents = []\n",
    "seen_pairs = set()  # Track (context, clause_type) to avoid exact duplicates\n",
    "\n",
    "for doc in data_entries:\n",
    "    title = doc.get('title', '')\n",
    "    \n",
    "    for para in doc.get('paragraphs', []):\n",
    "        context = para.get('context', '')\n",
    "        \n",
    "        # Collect multiple clause types from each contract\n",
    "        for qa in para.get('qas', []):\n",
    "            clause_type = qa.get('question', '')\n",
    "            is_impossible = qa.get('is_impossible', True)\n",
    "            \n",
    "            # Only take clauses that exist and haven't been seen before\n",
    "            pair = (context, clause_type)\n",
    "            if not is_impossible and pair not in seen_pairs:\n",
    "                documents.append({\n",
    "                    'document_title': title,\n",
    "                    'context': context,\n",
    "                    'clause_type': clause_type\n",
    "                })\n",
    "                seen_pairs.add(pair)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(documents)\n",
    "\n",
    "print(f\"✓ Created DataFrame with {len(df)} clause samples\")\n",
    "print(f\"\\nDataFrame Shape: {df.shape}\")\n",
    "print(f\"\\nColumn Names:\\n{df.columns.tolist()}\")\n",
    "print(f\"\\nUnique contracts: {df['context'].nunique()}\")\n",
    "print(f\"Unique clause types: {df['clause_type'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38b61748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "First 5 Rows of Dataset:\n",
      "================================================================================\n",
      "                                      document_title  \\\n",
      "0  LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...   \n",
      "1  LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...   \n",
      "2  LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...   \n",
      "3  LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...   \n",
      "4  LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...   \n",
      "\n",
      "                                             context  \\\n",
      "0  EXHIBIT 10.6\\n\\n                              ...   \n",
      "1  EXHIBIT 10.6\\n\\n                              ...   \n",
      "2  EXHIBIT 10.6\\n\\n                              ...   \n",
      "3  EXHIBIT 10.6\\n\\n                              ...   \n",
      "4  EXHIBIT 10.6\\n\\n                              ...   \n",
      "\n",
      "                                         clause_type  \n",
      "0  Highlight the parts (if any) of this contract ...  \n",
      "1  Highlight the parts (if any) of this contract ...  \n",
      "2  Highlight the parts (if any) of this contract ...  \n",
      "3  Highlight the parts (if any) of this contract ...  \n",
      "4  Highlight the parts (if any) of this contract ...  \n",
      "\n",
      "================================================================================\n",
      "Dataset Info:\n",
      "================================================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6693 entries, 0 to 6692\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   document_title  6693 non-null   object\n",
      " 1   context         6693 non-null   object\n",
      " 2   clause_type     6693 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 157.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Display first few rows\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"First 5 Rows of Dataset:\")\n",
    "print(\"=\"*80)\n",
    "print(df.head())\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Dataset Info:\")\n",
    "print(\"=\"*80)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09ad8e5",
   "metadata": {},
   "source": [
    "# 3. Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81dd5259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "document_title    0\n",
      "context           0\n",
      "clause_type       0\n",
      "dtype: int64\n",
      "\n",
      "Total samples: 6693\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(f\"\\nTotal samples: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8071f952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 clause types:\n",
      "clause_type\n",
      "Highlight the parts (if any) of this contract related to \"Document Name\" that should be reviewed by a lawyer. Details: The name of the contract                                                                                                                                                                       509\n",
      "Highlight the parts (if any) of this contract related to \"Parties\" that should be reviewed by a lawyer. Details: The two or more parties who signed the contract                                                                                                                                                      508\n",
      "Highlight the parts (if any) of this contract related to \"Agreement Date\" that should be reviewed by a lawyer. Details: The date of the contract                                                                                                                                                                      469\n",
      "Highlight the parts (if any) of this contract related to \"Governing Law\" that should be reviewed by a lawyer. Details: Which state/country's law governs the interpretation of the contract?                                                                                                                          436\n",
      "Highlight the parts (if any) of this contract related to \"Expiration Date\" that should be reviewed by a lawyer. Details: On what date will the contract's initial term expire?                                                                                                                                        412\n",
      "Highlight the parts (if any) of this contract related to \"Effective Date\" that should be reviewed by a lawyer. Details: The date when the contract is effective                                                                                                                                                       389\n",
      "Highlight the parts (if any) of this contract related to \"Anti-Assignment\" that should be reviewed by a lawyer. Details: Is consent or notice required of a party if the contract is assigned to a third party?                                                                                                       374\n",
      "Highlight the parts (if any) of this contract related to \"Cap On Liability\" that should be reviewed by a lawyer. Details: Does the contract include a cap on liability upon the breach of a party’s obligation? This includes time limitation for the counterparty to bring claims or maximum amount for recovery.    275\n",
      "Highlight the parts (if any) of this contract related to \"License Grant\" that should be reviewed by a lawyer. Details: Does the contract contain a license granted by one party to its counterparty?                                                                                                                  255\n",
      "Highlight the parts (if any) of this contract related to \"Audit Rights\" that should be reviewed by a lawyer. Details: Does a party have the right to  audit the books, records, or physical locations of the counterparty to ensure compliance with the contract?                                                     214\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution\n",
    "print(\"Top 10 clause types:\")\n",
    "print(df['clause_type'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee27a221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 clause types:\n",
      "clause_type\n",
      "Highlight the parts (if any) of this contract related to \"Document Name\" that should be reviewed by a lawyer. Details: The name of the contract                                                                                                                                                                       509\n",
      "Highlight the parts (if any) of this contract related to \"Parties\" that should be reviewed by a lawyer. Details: The two or more parties who signed the contract                                                                                                                                                      508\n",
      "Highlight the parts (if any) of this contract related to \"Agreement Date\" that should be reviewed by a lawyer. Details: The date of the contract                                                                                                                                                                      469\n",
      "Highlight the parts (if any) of this contract related to \"Governing Law\" that should be reviewed by a lawyer. Details: Which state/country's law governs the interpretation of the contract?                                                                                                                          436\n",
      "Highlight the parts (if any) of this contract related to \"Expiration Date\" that should be reviewed by a lawyer. Details: On what date will the contract's initial term expire?                                                                                                                                        412\n",
      "Highlight the parts (if any) of this contract related to \"Effective Date\" that should be reviewed by a lawyer. Details: The date when the contract is effective                                                                                                                                                       389\n",
      "Highlight the parts (if any) of this contract related to \"Anti-Assignment\" that should be reviewed by a lawyer. Details: Is consent or notice required of a party if the contract is assigned to a third party?                                                                                                       374\n",
      "Highlight the parts (if any) of this contract related to \"Cap On Liability\" that should be reviewed by a lawyer. Details: Does the contract include a cap on liability upon the breach of a party’s obligation? This includes time limitation for the counterparty to bring claims or maximum amount for recovery.    275\n",
      "Highlight the parts (if any) of this contract related to \"License Grant\" that should be reviewed by a lawyer. Details: Does the contract contain a license granted by one party to its counterparty?                                                                                                                  255\n",
      "Highlight the parts (if any) of this contract related to \"Audit Rights\" that should be reviewed by a lawyer. Details: Does a party have the right to  audit the books, records, or physical locations of the counterparty to ensure compliance with the contract?                                                     214\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Top clause types\n",
    "print(\"Top 10 clause types:\")\n",
    "print(df['clause_type'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41fac83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length: 10207 words\n",
      "Max length: 47733 words\n"
     ]
    }
   ],
   "source": [
    "# Check text lengths\n",
    "df['text_length'] = df['context'].apply(lambda x: len(str(x).split()))\n",
    "print(f\"Average length: {df['text_length'].mean():.0f} words\")\n",
    "print(f\"Max length: {df['text_length'].max()} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c66c8cd",
   "metadata": {},
   "source": [
    "# 4. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78ad9e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: THIS AGREEMENT is made on January 1, 2020!!!\n",
      "After: this agreement is made on january ,\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Basic text cleaning\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s\\.,;:\\-]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Test\n",
    "sample = \"THIS AGREEMENT is made on January 1, 2020!!!\"\n",
    "print(\"Before:\", sample)\n",
    "print(\"After:\", clean_text(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dbd3b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cleaned all documents\n"
     ]
    }
   ],
   "source": [
    "# Apply cleaning\n",
    "df['cleaned_text'] = df['context'].apply(clean_text)\n",
    "print(\"✓ Cleaned all documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f08f17",
   "metadata": {},
   "source": [
    "# 5. Text Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79dfab0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause length statistics:\n",
      "  Mean: 10201 words\n",
      "  Median: 7022 words\n",
      "  90th percentile: 24032 words\n",
      "  Max: 48116 words\n",
      "\n",
      "Most clause contexts are short (< 500 words), no truncation needed.\n"
     ]
    }
   ],
   "source": [
    "# Check clause context lengths\n",
    "df['clause_length'] = df['cleaned_text'].apply(lambda x: len(str(x).split()))\n",
    "print(f\"Clause length statistics:\")\n",
    "print(f\"  Mean: {df['clause_length'].mean():.0f} words\")\n",
    "print(f\"  Median: {df['clause_length'].median():.0f} words\")\n",
    "print(f\"  90th percentile: {df['clause_length'].quantile(0.90):.0f} words\")\n",
    "print(f\"  Max: {df['clause_length'].max()} words\")\n",
    "print(f\"\\nMost clause contexts are short (< 500 words), no truncation needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "302c0d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using 6693 clause contexts (no truncation needed)\n"
     ]
    }
   ],
   "source": [
    "# Use cleaned text directly (clause contexts are already short)\n",
    "df['sampled_text'] = df['cleaned_text']\n",
    "print(f\"✓ Using {len(df)} clause contexts (no truncation needed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627a4d8a",
   "metadata": {},
   "source": [
    "# 6. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd43de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    \"\"\"Simple tokenizer - built from scratch\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=10000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_to_index = {\"<OOV>\": 1}\n",
    "        self.word_counts = Counter()\n",
    "        \n",
    "    def fit_on_texts(self, texts):\n",
    "        for text in texts:\n",
    "            self.word_counts.update(str(text).split())\n",
    "        \n",
    "        most_common = self.word_counts.most_common(self.vocab_size - 2)\n",
    "        for idx, (word, _) in enumerate(most_common, start=2):\n",
    "            self.word_to_index[word] = idx\n",
    "        \n",
    "        print(f\"Vocabulary size: {len(self.word_to_index)}\")\n",
    "    \n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            seq = [self.word_to_index.get(word, 1) for word in str(text).split()]\n",
    "            sequences.append(seq)\n",
    "        return sequences\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.word_to_index)\n",
    "\n",
    "# Tokenizer will be built after filtering to top clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f146861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer setup deferred until after top-clause filtering\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer will be instantiated and fitted after filtering top clauses\n",
    "print(\"Tokenizer setup deferred until after top-clause filtering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b970fb9f",
   "metadata": {},
   "source": [
    "# 7. Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6128b78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences function (replaces Keras pad_sequences)\n",
    "def pad_sequences(sequences, maxlen, padding='post', value=0):\n",
    "    \"\"\"Pad sequences to the same length\"\"\"\n",
    "    padded = np.zeros((len(sequences), maxlen), dtype=np.int32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        if len(seq) > maxlen:\n",
    "            if padding == 'post':\n",
    "                padded[i] = seq[:maxlen]\n",
    "            else:\n",
    "                padded[i] = seq[-maxlen:]\n",
    "        else:\n",
    "            if padding == 'post':\n",
    "                padded[i, :len(seq)] = seq\n",
    "            else:\n",
    "                padded[i, -len(seq):] = seq\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a63819b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 3841 samples\n",
      "Top 10 clause types:\n",
      "  1. Highlight the parts (if any) of this contract related to \"Document Name\" that sh... (509 samples)\n",
      "  2. Highlight the parts (if any) of this contract related to \"Parties\" that should b... (508 samples)\n",
      "  3. Highlight the parts (if any) of this contract related to \"Agreement Date\" that s... (469 samples)\n",
      "  4. Highlight the parts (if any) of this contract related to \"Governing Law\" that sh... (436 samples)\n",
      "  5. Highlight the parts (if any) of this contract related to \"Expiration Date\" that ... (412 samples)\n",
      "  6. Highlight the parts (if any) of this contract related to \"Effective Date\" that s... (389 samples)\n",
      "  7. Highlight the parts (if any) of this contract related to \"Anti-Assignment\" that ... (374 samples)\n",
      "  8. Highlight the parts (if any) of this contract related to \"Cap On Liability\" that... (275 samples)\n",
      "  9. Highlight the parts (if any) of this contract related to \"License Grant\" that sh... (255 samples)\n",
      "  10. Highlight the parts (if any) of this contract related to \"Audit Rights\" that sho... (214 samples)\n",
      "Vocabulary size: 19999\n",
      "Vocabulary size: 19999\n",
      "Sequence length percentile(85th): 16784\n",
      "Max sequence length used: 192 (capped at 192)\n",
      "Padded shape (filtered): (3841, 192)\n",
      "Sequence length percentile(85th): 16784\n",
      "Max sequence length used: 192 (capped at 192)\n",
      "Padded shape (filtered): (3841, 192)\n"
     ]
    }
   ],
   "source": [
    "# Select top 10 clause types\n",
    "TOP_N = 10\n",
    "top_clauses = df['clause_type'].value_counts().head(TOP_N).index.tolist()\n",
    "df_filtered = df[df['clause_type'].isin(top_clauses)].copy()\n",
    "\n",
    "print(f\"Using {len(df_filtered)} samples\")\n",
    "print(f\"Top {TOP_N} clause types:\")\n",
    "for i, (clause, count) in enumerate(df['clause_type'].value_counts().head(TOP_N).items(), 1):\n",
    "    print(f\"  {i}. {clause[:80]}... ({count} samples)\")\n",
    "\n",
    "# Build tokenizer on filtered data with larger vocab\n",
    "tokenizer = CustomTokenizer(vocab_size=20000)\n",
    "tokenizer.fit_on_texts(df_filtered['sampled_text'])\n",
    "\n",
    "# Tokenize filtered data\n",
    "sequences_filtered = tokenizer.texts_to_sequences(df_filtered['sampled_text'])\n",
    "\n",
    "# Length stats and padding length\n",
    "sequence_lengths = [len(seq) for seq in sequences_filtered]\n",
    "percentile_len = int(np.percentile(sequence_lengths, 85))\n",
    "MAX_LENGTH = min(percentile_len, 192)\n",
    "print(f\"Sequence length percentile(85th): {percentile_len}\")\n",
    "print(f\"Max sequence length used: {MAX_LENGTH} (capped at 192)\")\n",
    "\n",
    "# Pad filtered sequences\n",
    "X_filtered = pad_sequences(sequences_filtered, maxlen=MAX_LENGTH, padding='post')\n",
    "print(f\"Padded shape (filtered): {X_filtered.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ce57e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV tokens: 405019 / 34389821 (1.18%)\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: OOV rate on filtered sequences\n",
    "# OOV token id is 1 in the tokenizer\n",
    "all_tokens = sum(len(seq) for seq in sequences_filtered)\n",
    "oov_tokens = sum(sum(1 for t in seq if t == 1) for seq in sequences_filtered)\n",
    "oov_pct = 100 * oov_tokens / max(1, all_tokens)\n",
    "print(f\"OOV tokens: {oov_tokens} / {all_tokens} ({oov_pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93d1f0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels shape: (3841,)\n",
      "Classes: ['Highlight the parts (if any) of this contract related to \"Agreement Date\" that should be reviewed by a lawyer. Details: The date of the contract'\n",
      " 'Highlight the parts (if any) of this contract related to \"Anti-Assignment\" that should be reviewed by a lawyer. Details: Is consent or notice required of a party if the contract is assigned to a third party?'\n",
      " 'Highlight the parts (if any) of this contract related to \"Audit Rights\" that should be reviewed by a lawyer. Details: Does a party have the right to\\xa0 audit the books, records, or physical locations of the counterparty to ensure compliance with the contract?'\n",
      " 'Highlight the parts (if any) of this contract related to \"Cap On Liability\" that should be reviewed by a lawyer. Details: Does the contract include a cap on liability upon the breach of a party’s obligation? This includes time limitation for the counterparty to bring claims or maximum amount for recovery.'\n",
      " 'Highlight the parts (if any) of this contract related to \"Document Name\" that should be reviewed by a lawyer. Details: The name of the contract'\n",
      " 'Highlight the parts (if any) of this contract related to \"Effective Date\" that should be reviewed by a lawyer. Details: The date when the contract is effective\\xa0'\n",
      " 'Highlight the parts (if any) of this contract related to \"Expiration Date\" that should be reviewed by a lawyer. Details: On what date will the contract\\'s initial term expire?'\n",
      " 'Highlight the parts (if any) of this contract related to \"Governing Law\" that should be reviewed by a lawyer. Details: Which state/country\\'s law governs the interpretation of the contract?'\n",
      " 'Highlight the parts (if any) of this contract related to \"License Grant\" that should be reviewed by a lawyer. Details: Does the contract contain a license granted by one party to its counterparty?'\n",
      " 'Highlight the parts (if any) of this contract related to \"Parties\" that should be reviewed by a lawyer. Details: The two or more parties who signed the contract']\n"
     ]
    }
   ],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(df_filtered['clause_type'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Labels shape: {y_encoded.shape}\")\n",
    "print(f\"Classes: {label_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f69fdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building TF-IDF matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Logistic accuracy (test): 0.0052\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       141\n",
      "           1     0.0000    0.0000    0.0000       112\n",
      "           2     0.0000    0.0000    0.0000        64\n",
      "           3     0.0000    0.0000    0.0000        83\n",
      "           4     0.0051    0.0131    0.0073       153\n",
      "           5     0.0000    0.0000    0.0000       117\n",
      "           6     0.0000    0.0000    0.0000       124\n",
      "           7     0.0000    0.0000    0.0000       131\n",
      "           8     0.0000    0.0000    0.0000        76\n",
      "           9     0.0090    0.0263    0.0134       152\n",
      "\n",
      "    accuracy                         0.0052      1153\n",
      "   macro avg     0.0014    0.0039    0.0021      1153\n",
      "weighted avg     0.0019    0.0052    0.0027      1153\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\heral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\heral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF + Logistic Regression baseline (quick sanity check)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "texts = df_filtered['sampled_text'].astype(str).tolist()\n",
    "labels = y_encoded\n",
    "\n",
    "print('Building TF-IDF matrix...')\n",
    "vect = TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n",
    "X_tfidf = vect.fit_transform(texts)\n",
    "\n",
    "# Split and train a simple linear classifier\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_tfidf, labels, test_size=0.30, random_state=42, stratify=labels)\n",
    "clf = LogisticRegression(max_iter=2000, solver='lbfgs', multi_class='multinomial')\n",
    "clf.fit(X_tr, y_tr)\n",
    "acc = clf.score(X_te, y_te)\n",
    "print(f\"TF-IDF Logistic accuracy (test): {acc:.4f}\")\n",
    "\n",
    "# Print detailed per-class report\n",
    "y_pred = clf.predict(X_te)\n",
    "print('\\nClassification report:')\n",
    "print(classification_report(y_te, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da76fde",
   "metadata": {},
   "source": [
    "# 8. Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c01355c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (2688, 192)\n",
      "Val: (576, 192)\n",
      "Test: (577, 192)\n",
      "Class counts: [328 262 150 192 356 272 288 305 179 356]\n",
      "Class weights (normalized): [0.7551622  0.94539389 1.65128799 1.29006875 0.69576742 0.91063676\n",
      " 0.86004583 0.81210885 1.38376089 0.69576742]\n"
     ]
    }
   ],
   "source": [
    "# Split data: 70% train, 15% val, 15% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_filtered, y_encoded, test_size=0.30, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}\")\n",
    "print(f\"Val: {X_val.shape}\")\n",
    "print(f\"Test: {X_test.shape}\")\n",
    "\n",
    "# Class weights to handle imbalance (toggle with USE_CLASS_WEIGHTS)\n",
    "class_counts = np.bincount(y_train, minlength=num_classes)\n",
    "class_weights = 1.0 / (class_counts + 1e-6)\n",
    "class_weights = class_weights * (num_classes / class_weights.sum())\n",
    "print(\"Class counts:\", class_counts)\n",
    "print(\"Class weights (normalized):\", class_weights)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "USE_CLASS_WEIGHTS = True\n",
    "USE_SAMPLER = False\n",
    "\n",
    "class ClauseDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = ClauseDataset(X_train, y_train)\n",
    "val_dataset = ClauseDataset(X_val, y_val)\n",
    "test_dataset = ClauseDataset(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48211ecb",
   "metadata": {},
   "source": [
    "# 9. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76b5c1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: 19999, Classes: 10, Max length: 192\n"
     ]
    }
   ],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"Bidirectional stacked LSTM for clause classification\"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim=200, lstm_1=128, lstm_2=96, dropout=0.25, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embed_dim, padding_idx=0)\n",
    "        self.lstm1 = nn.LSTM(embed_dim, lstm_1, batch_first=True, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.lstm2 = nn.LSTM(lstm_1 * 2, lstm_2, batch_first=True, bidirectional=True)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(lstm_2 * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, (h_n, _) = self.lstm2(x)\n",
    "        x = self.dropout2(h_n[-2:].transpose(0,1).reshape(x.size(0), -1))\n",
    "        return self.fc(x)\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.word_to_index)\n",
    "NUM_CLASSES = num_classes\n",
    "print(f\"Vocab: {VOCAB_SIZE}, Classes: {NUM_CLASSES}, Max length: {MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5448ff7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMClassifier(\n",
      "  (embedding): Embedding(20000, 200, padding_idx=0)\n",
      "  (lstm1): LSTM(200, 128, batch_first=True, bidirectional=True)\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (lstm2): LSTM(256, 96, batch_first=True, bidirectional=True)\n",
      "  (dropout2): Dropout(p=0.25, inplace=False)\n",
      "  (fc): Linear(in_features=192, out_features=10, bias=True)\n",
      ")\n",
      "Total parameters: 4611722\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model (demo shape/params)\n",
    "model = LSTMClassifier(VOCAB_SIZE, embed_dim=200, num_classes=NUM_CLASSES).to(device)\n",
    "print(model)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a147b",
   "metadata": {},
   "source": [
    "# 10. Hyperparameter Tuning Setup\n",
    "\n",
    "Testing different optimizers as required by the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5d92581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing optimizers: Adam, RMSprop, SGD\n",
      "Learning rates: 0.001, 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Optimizers to test\n",
    "print(\"Testing optimizers: Adam, RMSprop, SGD\")\n",
    "print(\"Learning rates: 0.001, 0.0001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4756ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will test 4 configurations\n"
     ]
    }
   ],
   "source": [
    "# Configurations to test - gentler LRs and larger batch for stability\n",
    "configs = [\n",
    "    {'opt': 'Adam',    'lr': 0.0005, 'wd': 1e-4, 'batch': 64, 'epochs': 25},\n",
    "    {'opt': 'Adam',    'lr': 0.0008, 'wd': 1e-4, 'batch': 64, 'epochs': 25},\n",
    "    {'opt': 'Adam',    'lr': 0.0003, 'wd': 1e-4, 'batch': 64, 'epochs': 25},\n",
    "    {'opt': 'RMSprop', 'lr': 0.0005, 'wd': 0.0,  'batch': 64, 'epochs': 25},\n",
    "]\n",
    "\n",
    "print(f\"Will test {len(configs)} configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61058ec0",
   "metadata": {},
   "source": [
    "# 11. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "751b7ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "models_dir = r'd:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models'\n",
    "os.makedirs(models_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba2d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Config 1/4: Adam, LR=0.0015, WD=0.0\n",
      "============================================================\n",
      "Model created, starting training...\n",
      "Training batches: 56, Val batches: 12\n",
      "Training batches: 56, Val batches: 12\n",
      "Epoch 1/20 - Train loss 2.2547, acc 0.1071 | Val loss 2.3564, acc 0.0538\n",
      "Epoch 1/20 - Train loss 2.2547, acc 0.1071 | Val loss 2.3564, acc 0.0538\n",
      "Epoch 2/20 - Train loss 2.1862, acc 0.1600 | Val loss 2.4802, acc 0.0417\n",
      "Epoch 2/20 - Train loss 2.1862, acc 0.1600 | Val loss 2.4802, acc 0.0417\n",
      "Epoch 3/20 - Train loss 2.1037, acc 0.1927 | Val loss 2.6238, acc 0.0365\n",
      "Epoch 3/20 - Train loss 2.1037, acc 0.1927 | Val loss 2.6238, acc 0.0365\n",
      "Epoch 4/20 - Train loss 2.0791, acc 0.2050 | Val loss 2.7054, acc 0.0191\n",
      "Epoch 4/20 - Train loss 2.0791, acc 0.2050 | Val loss 2.7054, acc 0.0191\n",
      "Epoch 5/20 - Train loss 2.0274, acc 0.2217 | Val loss 2.7943, acc 0.0156\n",
      "Epoch 5/20 - Train loss 2.0274, acc 0.2217 | Val loss 2.7943, acc 0.0156\n",
      "Epoch 6/20 - Train loss 1.9846, acc 0.2128 | Val loss 2.9450, acc 0.0191\n",
      "Epoch 6/20 - Train loss 1.9846, acc 0.2128 | Val loss 2.9450, acc 0.0191\n",
      "Epoch 7/20 - Train loss 1.9340, acc 0.2422 | Val loss 3.0279, acc 0.0069\n",
      "Early stopping at epoch 7\n",
      "Epoch 7/20 - Train loss 1.9340, acc 0.2422 | Val loss 3.0279, acc 0.0069\n",
      "Early stopping at epoch 7\n",
      "Val pred distribution: Counter({np.int64(2): 163, np.int64(3): 119, np.int64(8): 101, np.int64(1): 53, np.int64(9): 34, np.int64(6): 30, np.int64(7): 29, np.int64(5): 28, np.int64(0): 10, np.int64(4): 9})\n",
      "Val pred distribution: Counter({np.int64(2): 163, np.int64(3): 119, np.int64(8): 101, np.int64(1): 53, np.int64(9): 34, np.int64(6): 30, np.int64(7): 29, np.int64(5): 28, np.int64(0): 10, np.int64(4): 9})\n",
      "Test accuracy: 0.0087\n",
      "Test accuracy: 0.0087\n",
      "Saved: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models\\model_1.pt and d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models\\model_1.h5\n",
      "\n",
      "============================================================\n",
      "Config 2/4: Adam, LR=0.001, WD=0.0\n",
      "============================================================\n",
      "Model created, starting training...\n",
      "Training batches: 56, Val batches: 12\n",
      "Saved: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models\\model_1.pt and d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models\\model_1.h5\n",
      "\n",
      "============================================================\n",
      "Config 2/4: Adam, LR=0.001, WD=0.0\n",
      "============================================================\n",
      "Model created, starting training...\n",
      "Training batches: 56, Val batches: 12\n",
      "Epoch 1/20 - Train loss 2.2535, acc 0.1164 | Val loss 2.4060, acc 0.0573\n",
      "Epoch 1/20 - Train loss 2.2535, acc 0.1164 | Val loss 2.4060, acc 0.0573\n",
      "Epoch 2/20 - Train loss 2.1860, acc 0.1525 | Val loss 2.4464, acc 0.0417\n",
      "Epoch 2/20 - Train loss 2.1860, acc 0.1525 | Val loss 2.4464, acc 0.0417\n",
      "Epoch 3/20 - Train loss 2.1442, acc 0.1927 | Val loss 2.5382, acc 0.0399\n",
      "Epoch 3/20 - Train loss 2.1442, acc 0.1927 | Val loss 2.5382, acc 0.0399\n",
      "Epoch 4/20 - Train loss 2.0697, acc 0.2113 | Val loss 2.6803, acc 0.0278\n",
      "Epoch 4/20 - Train loss 2.0697, acc 0.2113 | Val loss 2.6803, acc 0.0278\n",
      "Epoch 5/20 - Train loss 2.0404, acc 0.2139 | Val loss 2.7509, acc 0.0139\n",
      "Epoch 5/20 - Train loss 2.0404, acc 0.2139 | Val loss 2.7509, acc 0.0139\n",
      "Epoch 6/20 - Train loss 2.0103, acc 0.2158 | Val loss 2.8502, acc 0.0226\n",
      "Epoch 6/20 - Train loss 2.0103, acc 0.2158 | Val loss 2.8502, acc 0.0226\n",
      "Epoch 7/20 - Train loss 1.9868, acc 0.2240 | Val loss 2.8881, acc 0.0208\n",
      "Early stopping at epoch 7\n",
      "Epoch 7/20 - Train loss 1.9868, acc 0.2240 | Val loss 2.8881, acc 0.0208\n",
      "Early stopping at epoch 7\n",
      "Val pred distribution: Counter({np.int64(2): 141, np.int64(8): 132, np.int64(3): 112, np.int64(7): 66, np.int64(6): 31, np.int64(9): 27, np.int64(1): 27, np.int64(4): 17, np.int64(5): 14, np.int64(0): 9})\n",
      "Val pred distribution: Counter({np.int64(2): 141, np.int64(8): 132, np.int64(3): 112, np.int64(7): 66, np.int64(6): 31, np.int64(9): 27, np.int64(1): 27, np.int64(4): 17, np.int64(5): 14, np.int64(0): 9})\n",
      "Test accuracy: 0.0121\n",
      "Saved: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models\\model_2.pt and d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models\\model_2.h5\n",
      "\n",
      "============================================================\n",
      "Config 3/4: Adam, LR=0.0008, WD=0.0\n",
      "============================================================\n",
      "Model created, starting training...\n",
      "Training batches: 56, Val batches: 12\n",
      "Test accuracy: 0.0121\n",
      "Saved: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models\\model_2.pt and d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models\\model_2.h5\n",
      "\n",
      "============================================================\n",
      "Config 3/4: Adam, LR=0.0008, WD=0.0\n",
      "============================================================\n",
      "Model created, starting training...\n",
      "Training batches: 56, Val batches: 12\n",
      "Epoch 1/20 - Train loss 2.2534, acc 0.1124 | Val loss 2.3407, acc 0.0625\n",
      "Epoch 1/20 - Train loss 2.2534, acc 0.1124 | Val loss 2.3407, acc 0.0625\n",
      "Epoch 2/20 - Train loss 2.2209, acc 0.1306 | Val loss 2.4309, acc 0.0451\n",
      "Epoch 2/20 - Train loss 2.2209, acc 0.1306 | Val loss 2.4309, acc 0.0451\n",
      "Epoch 3/20 - Train loss 2.1562, acc 0.1749 | Val loss 2.4849, acc 0.0330\n",
      "Epoch 3/20 - Train loss 2.1562, acc 0.1749 | Val loss 2.4849, acc 0.0330\n",
      "Epoch 4/20 - Train loss 2.1389, acc 0.1793 | Val loss 2.5809, acc 0.0312\n",
      "Epoch 4/20 - Train loss 2.1389, acc 0.1793 | Val loss 2.5809, acc 0.0312\n",
      "Epoch 5/20 - Train loss 2.0652, acc 0.2147 | Val loss 2.6338, acc 0.0243\n",
      "Epoch 5/20 - Train loss 2.0652, acc 0.2147 | Val loss 2.6338, acc 0.0243\n",
      "Epoch 6/20 - Train loss 2.0263, acc 0.2184 | Val loss 2.7379, acc 0.0208\n",
      "Epoch 6/20 - Train loss 2.0263, acc 0.2184 | Val loss 2.7379, acc 0.0208\n",
      "Epoch 7/20 - Train loss 1.9995, acc 0.2191 | Val loss 2.8442, acc 0.0156\n",
      "Early stopping at epoch 7\n",
      "Epoch 7/20 - Train loss 1.9995, acc 0.2191 | Val loss 2.8442, acc 0.0156\n",
      "Early stopping at epoch 7\n",
      "Val pred distribution: Counter({np.int64(2): 170, np.int64(8): 145, np.int64(3): 90, np.int64(5): 67, np.int64(0): 30, np.int64(1): 30, np.int64(7): 21, np.int64(9): 10, np.int64(4): 7, np.int64(6): 6})\n",
      "Val pred distribution: Counter({np.int64(2): 170, np.int64(8): 145, np.int64(3): 90, np.int64(5): 67, np.int64(0): 30, np.int64(1): 30, np.int64(7): 21, np.int64(9): 10, np.int64(4): 7, np.int64(6): 6})\n",
      "Test accuracy: 0.0156\n",
      "Saved: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models\\model_3.pt and d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models\\model_3.h5\n",
      "\n",
      "============================================================\n",
      "Config 4/4: RMSprop, LR=0.001, WD=0.0\n",
      "============================================================\n",
      "Model created, starting training...\n",
      "Training batches: 56, Val batches: 12\n",
      "Test accuracy: 0.0156\n",
      "Saved: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models\\model_3.pt and d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models\\model_3.h5\n",
      "\n",
      "============================================================\n",
      "Config 4/4: RMSprop, LR=0.001, WD=0.0\n",
      "============================================================\n",
      "Model created, starting training...\n",
      "Training batches: 56, Val batches: 12\n",
      "Epoch 1/18 - Train loss 2.2268, acc 0.1388 | Val loss 2.4492, acc 0.0399\n",
      "Epoch 1/18 - Train loss 2.2268, acc 0.1388 | Val loss 2.4492, acc 0.0399\n",
      "Epoch 2/18 - Train loss 2.1367, acc 0.1908 | Val loss 2.5497, acc 0.0243\n",
      "Epoch 2/18 - Train loss 2.1367, acc 0.1908 | Val loss 2.5497, acc 0.0243\n",
      "Epoch 3/18 - Train loss 2.1053, acc 0.2061 | Val loss 2.6114, acc 0.0278\n",
      "Epoch 3/18 - Train loss 2.1053, acc 0.2061 | Val loss 2.6114, acc 0.0278\n",
      "Epoch 4/18 - Train loss 2.0613, acc 0.2106 | Val loss 2.7003, acc 0.0174\n",
      "Epoch 4/18 - Train loss 2.0613, acc 0.2106 | Val loss 2.7003, acc 0.0174\n",
      "Epoch 5/18 - Train loss 1.9921, acc 0.2254 | Val loss 2.8023, acc 0.0156\n",
      "Epoch 5/18 - Train loss 1.9921, acc 0.2254 | Val loss 2.8023, acc 0.0156\n",
      "Epoch 6/18 - Train loss 1.9651, acc 0.2366 | Val loss 2.9065, acc 0.0104\n",
      "Epoch 6/18 - Train loss 1.9651, acc 0.2366 | Val loss 2.9065, acc 0.0104\n",
      "Epoch 7/18 - Train loss 1.9495, acc 0.2303 | Val loss 2.9784, acc 0.0104\n",
      "Early stopping at epoch 7\n",
      "Epoch 7/18 - Train loss 1.9495, acc 0.2303 | Val loss 2.9784, acc 0.0104\n",
      "Early stopping at epoch 7\n",
      "Val pred distribution: Counter({np.int64(2): 177, np.int64(3): 117, np.int64(8): 103, np.int64(5): 57, np.int64(7): 37, np.int64(1): 29, np.int64(9): 24, np.int64(6): 24, np.int64(4): 4, np.int64(0): 4})\n",
      "Val pred distribution: Counter({np.int64(2): 177, np.int64(3): 117, np.int64(8): 103, np.int64(5): 57, np.int64(7): 37, np.int64(1): 29, np.int64(9): 24, np.int64(6): 24, np.int64(4): 4, np.int64(0): 4})\n",
      "Test accuracy: 0.0087\n",
      "Saved: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models\\model_4.pt and d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models\\model_4.h5\n",
      "\n",
      "✓ Training complete!\n",
      "Test accuracy: 0.0087\n",
      "Saved: d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models\\model_4.pt and d:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models\\model_4.h5\n",
      "\n",
      "✓ Training complete!\n"
     ]
    }
   ],
   "source": [
    "def run_epoch(model, loader, criterion, optimizer=None):\n",
    "    model.train() if optimizer else model.eval()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(loader):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        if optimizer:\n",
    "            optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        if optimizer:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item() * X_batch.size(0)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        total_correct += (preds == y_batch).sum().item()\n",
    "        total_samples += X_batch.size(0)\n",
    "        \n",
    "        # Progress indicator every 50 batches\n",
    "        if optimizer and batch_idx % 50 == 0:\n",
    "            print(f\"  Batch {batch_idx}/{len(loader)}\", end='\\r')\n",
    "    \n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = total_correct / total_samples\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "def save_model_as_h5(model, filepath):\n",
    "    \"\"\"Save PyTorch model weights to HDF5 format\"\"\"\n",
    "    import h5py\n",
    "    state_dict = model.state_dict()\n",
    "    with h5py.File(filepath, 'w') as f:\n",
    "        for key, value in state_dict.items():\n",
    "            f.create_dataset(key, data=value.cpu().numpy())\n",
    "\n",
    "results = []\n",
    "models_dir = r'd:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "for i, cfg in enumerate(configs, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Config {i}/{len(configs)}: {cfg['opt']}, LR={cfg['lr']}, WD={cfg['wd']}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    model = LSTMClassifier(VOCAB_SIZE, embed_dim=200, num_classes=NUM_CLASSES).to(device)\n",
    "    print(f\"Model created, starting training...\")\n",
    "    \n",
    "    if cfg['opt'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=cfg['lr'], weight_decay=cfg.get('wd', 0.0))\n",
    "    elif cfg['opt'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=cfg['lr'], weight_decay=cfg.get('wd', 0.0))\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=cfg['lr'], momentum=0.9, weight_decay=cfg.get('wd', 0.0))\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor if USE_CLASS_WEIGHTS else None)\n",
    "    \n",
    "    if USE_SAMPLER:\n",
    "        sample_weights = class_weights_tensor.cpu().numpy()[y_train]\n",
    "        train_sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=cfg['batch'], sampler=train_sampler)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=cfg['batch'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg['batch'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=cfg['batch'], shuffle=False)\n",
    "    \n",
    "    print(f\"Training batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 6\n",
    "    \n",
    "    for epoch in range(cfg['epochs']):\n",
    "        train_loss, train_acc = run_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_acc = run_epoch(model, val_loader, criterion, optimizer=None)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{cfg['epochs']} - Train loss {train_loss:.4f}, acc {train_acc:.4f} | Val loss {val_loss:.4f}, acc {val_acc:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Quick val prediction distribution\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_val_preds = []\n",
    "        for Xb, _ in val_loader:\n",
    "            Xb = Xb.to(device)\n",
    "            preds = model(Xb).argmax(dim=1).cpu().numpy()\n",
    "            all_val_preds.extend(preds)\n",
    "    from collections import Counter\n",
    "    pred_dist = Counter(all_val_preds)\n",
    "    print(f\"Val pred distribution: {pred_dist}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = run_epoch(model, test_loader, criterion, optimizer=None)\n",
    "    results.append({\n",
    "        'config': i,\n",
    "        'optimizer': cfg['opt'],\n",
    "        'lr': cfg['lr'],\n",
    "        'wd': cfg.get('wd', 0.0),\n",
    "        'batch_size': cfg['batch'],\n",
    "        'train_acc': train_acc,\n",
    "        'val_acc': val_acc,\n",
    "        'test_acc': test_acc\n",
    "    })\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Save model in both PyTorch (.pt) and HDF5 (.h5) formats\n",
    "    pt_path = os.path.join(models_dir, f'model_{i}.pt')\n",
    "    h5_path = os.path.join(models_dir, f'model_{i}.h5')\n",
    "    torch.save(model.state_dict(), pt_path)\n",
    "    save_model_as_h5(model, h5_path)\n",
    "    print(f\"Saved: {pt_path} and {h5_path}\")\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0054f0",
   "metadata": {},
   "source": [
    "# 12. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f42d3a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Results:\n",
      "   config optimizer      lr   wd  batch_size  train_acc   val_acc  test_acc\n",
      "0       1      Adam  0.0015  0.0          48   0.242188  0.006944  0.008666\n",
      "1       2      Adam  0.0010  0.0          48   0.223958  0.020833  0.012132\n",
      "2       3      Adam  0.0008  0.0          48   0.219122  0.015625  0.015598\n",
      "3       4   RMSprop  0.0010  0.0          48   0.230283  0.010417  0.008666\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(r'd:\\CodingRelated\\Codes.Ams\\ANNFINAL\\experiment_results.csv', index=False)\n",
    "\n",
    "print(\"All Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c3d41f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BEST MODEL\n",
      "============================================================\n",
      "Optimizer: Adam\n",
      "Learning Rate: 0.0008\n",
      "Test Accuracy: 1.56%\n",
      "\n",
      "✗ Below 50%\n"
     ]
    }
   ],
   "source": [
    "# Best model\n",
    "best_idx = results_df['test_acc'].idxmax()\n",
    "best = results_df.iloc[best_idx]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BEST MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Optimizer: {best['optimizer']}\")\n",
    "print(f\"Learning Rate: {best['lr']}\")\n",
    "print(f\"Test Accuracy: {best['test_acc']:.2%}\")\n",
    "\n",
    "if best['test_acc'] >= 0.50:\n",
    "    print(\"\\n✓ Meets 50% requirement!\")\n",
    "else:\n",
    "    print(\"\\n✗ Below 50%\")\n",
    "\n",
    "best_model_path = os.path.join(models_dir, f\"model_{best_idx + 1}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb130d8",
   "metadata": {},
   "source": [
    "# 13. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "022d710b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model from: model_3.pt\n"
     ]
    }
   ],
   "source": [
    "# Load best model (match training embed_dim)\n",
    "best_model = LSTMClassifier(VOCAB_SIZE, embed_dim=200, num_classes=NUM_CLASSES).to(device)\n",
    "best_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "best_model.eval()\n",
    "\n",
    "# Get predictions\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.long).to(device)\n",
    "with torch.no_grad():\n",
    "    y_pred = best_model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = y_test\n",
    "\n",
    "print(f\"Loaded best model from: model_{best_idx + 1}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fca778d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[ 1  0  0  0 18  0  0  0  0 52]\n",
      " [ 0  0  0  0 12  0  0  0  0 44]\n",
      " [ 0  0  0  0  5  0  0  0  0 27]\n",
      " [ 0  0  0  0  4  0  0  0  0 37]\n",
      " [ 0  0  0  0  7  0  0  0  0 70]\n",
      " [ 0  0  0  0  9  0  0  0  0 49]\n",
      " [ 0  0  0  0  7  0  0  0  0 55]\n",
      " [ 0  0  0  0  8  0  0  0  0 58]\n",
      " [ 0  0  0  0  8  0  0  0  0 30]\n",
      " [ 0  0  0  0 25  0  0  1  0 50]]\n",
      "\n",
      "Accuracy per class:\n",
      "Highlight the parts (if any) of this contract related to \"Agreement Date\" that should be reviewed by a lawyer. Details: The date of the contract: 1.41%\n",
      "Highlight the parts (if any) of this contract related to \"Anti-Assignment\" that should be reviewed by a lawyer. Details: Is consent or notice required of a party if the contract is assigned to a third party?: 0.00%\n",
      "Highlight the parts (if any) of this contract related to \"Audit Rights\" that should be reviewed by a lawyer. Details: Does a party have the right to  audit the books, records, or physical locations of the counterparty to ensure compliance with the contract?: 0.00%\n",
      "Highlight the parts (if any) of this contract related to \"Cap On Liability\" that should be reviewed by a lawyer. Details: Does the contract include a cap on liability upon the breach of a party’s obligation? This includes time limitation for the counterparty to bring claims or maximum amount for recovery.: 0.00%\n",
      "Highlight the parts (if any) of this contract related to \"Document Name\" that should be reviewed by a lawyer. Details: The name of the contract: 9.09%\n",
      "Highlight the parts (if any) of this contract related to \"Effective Date\" that should be reviewed by a lawyer. Details: The date when the contract is effective : 0.00%\n",
      "Highlight the parts (if any) of this contract related to \"Expiration Date\" that should be reviewed by a lawyer. Details: On what date will the contract's initial term expire?: 0.00%\n",
      "Highlight the parts (if any) of this contract related to \"Governing Law\" that should be reviewed by a lawyer. Details: Which state/country's law governs the interpretation of the contract?: 0.00%\n",
      "Highlight the parts (if any) of this contract related to \"License Grant\" that should be reviewed by a lawyer. Details: Does the contract contain a license granted by one party to its counterparty?: 0.00%\n",
      "Highlight the parts (if any) of this contract related to \"Parties\" that should be reviewed by a lawyer. Details: The two or more parties who signed the contract: 65.79%\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix - save without displaying to avoid matplotlib issues\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "# Print confusion matrix as text\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nAccuracy per class:\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    class_acc = cm[i, i] / cm[i].sum() if cm[i].sum() > 0 else 0\n",
    "    print(f\"{class_name}: {class_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5308458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "                                                                                                                                                                                                                                                                                                                    precision    recall  f1-score   support\n",
      "\n",
      "                                                                                                                                                                  Highlight the parts (if any) of this contract related to \"Agreement Date\" that should be reviewed by a lawyer. Details: The date of the contract       1.00      0.01      0.03        71\n",
      "                                                                                                   Highlight the parts (if any) of this contract related to \"Anti-Assignment\" that should be reviewed by a lawyer. Details: Is consent or notice required of a party if the contract is assigned to a third party?       0.00      0.00      0.00        56\n",
      "                                                 Highlight the parts (if any) of this contract related to \"Audit Rights\" that should be reviewed by a lawyer. Details: Does a party have the right to  audit the books, records, or physical locations of the counterparty to ensure compliance with the contract?       0.00      0.00      0.00        32\n",
      "Highlight the parts (if any) of this contract related to \"Cap On Liability\" that should be reviewed by a lawyer. Details: Does the contract include a cap on liability upon the breach of a party’s obligation? This includes time limitation for the counterparty to bring claims or maximum amount for recovery.       0.00      0.00      0.00        41\n",
      "                                                                                                                                                                   Highlight the parts (if any) of this contract related to \"Document Name\" that should be reviewed by a lawyer. Details: The name of the contract       0.07      0.09      0.08        77\n",
      "                                                                                                                                                  Highlight the parts (if any) of this contract related to \"Effective Date\" that should be reviewed by a lawyer. Details: The date when the contract is effective        0.00      0.00      0.00        58\n",
      "                                                                                                                                    Highlight the parts (if any) of this contract related to \"Expiration Date\" that should be reviewed by a lawyer. Details: On what date will the contract's initial term expire?       0.00      0.00      0.00        62\n",
      "                                                                                                                      Highlight the parts (if any) of this contract related to \"Governing Law\" that should be reviewed by a lawyer. Details: Which state/country's law governs the interpretation of the contract?       0.00      0.00      0.00        66\n",
      "                                                                                                              Highlight the parts (if any) of this contract related to \"License Grant\" that should be reviewed by a lawyer. Details: Does the contract contain a license granted by one party to its counterparty?       0.00      0.00      0.00        38\n",
      "                                                                                                                                                  Highlight the parts (if any) of this contract related to \"Parties\" that should be reviewed by a lawyer. Details: The two or more parties who signed the contract       0.11      0.66      0.18        76\n",
      "\n",
      "                                                                                                                                                                                                                                                                                                          accuracy                           0.10       577\n",
      "                                                                                                                                                                                                                                                                                                         macro avg       0.12      0.08      0.03       577\n",
      "                                                                                                                                                                                                                                                                                                      weighted avg       0.15      0.10      0.04       577\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\heral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\heral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_classes, y_pred_classes, \n",
    "                          target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a4eacc",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "## Problem\n",
    "Legal contract clause classification — automate categorization of clauses (e.g., governing law, termination, confidentiality).\n",
    "\n",
    "## Solution\n",
    "Stacked bidirectional LSTM (2 layers) on cleaned clause text (inputs capped at 512 tokens).\n",
    "\n",
    "## Dataset\n",
    "CUAD v1 — ~13k labeled clauses from 510 public contracts; using top 10 clause types.\n",
    "\n",
    "## Network Structure\n",
    "- Embedding layer (128 dims)\n",
    "- BiLSTM layer 1 (64 units per direction) + dropout 0.3\n",
    "- BiLSTM layer 2 (48 units per direction) + dropout 0.3\n",
    "- Dense output (10 classes, softmax)\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "Tested Adam and RMSprop with multiple learning rates; batch size 32; early stopping (patience=4).\n",
    "\n",
    "## Results\n",
    "- Best optimizer: [see above]\n",
    "- Test accuracy: [see above]\n",
    "- Status: target 50–60% accuracy\n",
    "\n",
    "## Tools Used\n",
    "- Python, PyTorch (from-scratch training), NumPy, Pandas, Scikit-learn\n",
    "\n",
    "All training done from scratch — no pretrained models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
